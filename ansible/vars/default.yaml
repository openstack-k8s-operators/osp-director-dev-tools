---
base_path: /home/ocp
#dev_scripts_repo: defaults to "https://github.com/openshift-metal3/dev-scripts.git"
#dev_scripts_branch: defaults to "HEAD"

# To set a specific release to install.
ocp_version: 4.7
ocp_release_image: quay.io/openshift-release-dev/ocp-release:4.7.4-x86_64
ocp_release_type: ga

# If set to true:
# - Assisted installer is used to install OCP instead of dev-scripts
# - Thus, dev-scripts related variables are ignored, and ...
# - ocp_release_type is ignored
# - ocp_version must be 4.7 (for now)
# - ocp_release_image is ignored (for now)
# - Other OCP AI variables are stored in vars/ocp_ai.yaml to reduce clutter in this file
# NOTE: This variable should be set to true in local-defaults.yaml as opposed to changing it here,
#       otherwise our playbooks seem to be flaky with executing the AI logic
#ocp_ai: false 

# private repo to read required secret files from
# The playbooks expect secret files which can be placed in a private repo
# Right now these are:
# * rhel-subscription.yaml - content:
#   rhel_subscription_activation_key: <activation key>
#   rhel_subscription_org_id: "xxxxxxx"
# * pull-secret
#   obtain it from https://cloud.redhat.com/openshift/install/pull-secret
#secrets_repo: https uri to repo, no default manual local file is expected if not present
#secrets_branch: defaults to "HEAD"

# dev scripts switched to ipv6 per default, for now switch back
# https://github.com/openshift-metal3/dev-scripts/pull/969
ocp_ip_stack: v4

# size the OCP VM resources
ocp_num_masters: 3
ocp_num_workers: 3
# number of extra VMs to create but not deploy, used as OSP computes
ocp_num_extra_workers: 2
ocp_extra_workers_online_status: false
ocp_master_memory: 16384
ocp_master_vcpu: 8
ocp_master_disk: 30
ocp_worker_memory: 30000
ocp_worker_vcpu: 8
ocp_worker_disk: 50

# OCP cluster name
ocp_cluster_name: ostest
#ocp_domain_name: defaults to test.metalkube.org

# Released version of the opm package (can be set to 'latest')
opm_version: v1.12.5

# operator-sdk version to use (must be specific version)
sdk_version: v1.5.0

# kustomize version to use (must be specific version)
kustomize_version: v4.0.1

# kuttl version to use (must be specific version)
kuttl_version: 0.9.0

# SRIOV network operator version (usually should correspond to X.X release)
sriov_version: 4.7

# Performance addon operator version (usually should correspond to X.X release)
perf_version: 4.7

# CNV addon operator version (usually should correspond to X.X release)
cnv_hyperconverged_operator_version: 2.6.1

# namespace to deploy the operator to
# Note: right now only openstack is supported as it is hardcoded in the Dockerfile
namespace: openstack
watch_namespace: openstack,openshift-machine-api,openshift-sriov-network-operator

# operator github url where operators repos are underneath - default https://github.com/openstack-k8s-operators
#openstack_k8s_operators_https_url: defaults to "https://github.com/openstack-k8s-operators"
#openstack_k8s_operators_director_branch: defaults to "HEAD"

# osp-director-operator image and tag to use
director_operator_image: quay.io/openstack-k8s-operators/osp-director-operator
director_operator_version: 0.0.1
csv_version: 0.0.1

#ocp_network_type: OVNKubernetes
ocp_network_type: OpenShiftSDN

# Tempest
tempest_enabled: false

# tempest timeout in seconds
tempest_timeout: 3600

# tempest test whitelist, if [] smoke test runs
#tempest_whitelist: []
tempest_whitelist:
  - 'tempest.api.compute.servers.test_create_server.ServersTestJSON.test_list_servers'

local_working_dir: "~/{{ ocp_cluster_name }}-working"

# nfs export directory
nfs_data_dir: /home/nfs/data
nfs_export_dir: /home/nfs

default_timeout: 240

# openstackclient container image
openstackclient_image: quay.io/openstack-k8s-operators/rhosp16-openstack-tripleoclient:16.2_20210521.1

# OSP controller VM sizing
osp_controller_count: 1
osp_controller_cores: 6
osp_controller_memory: 12
osp_controller_disk_size: 40
# use http://download.devel.redhat.com to get the local mirror depending on where the server is
osp_controller_base_image_url: http://download.devel.redhat.com/brewroot/packages/rhel-guest-image/8.4/825/images/rhel-guest-image-8.4-825.x86_64.qcow2
osp_controller_storage_class: host-nfs-storageclass
# Interface connected to osp network, will be configured as linux-bridge using nmstate
osp_interface: enp7s0
osp_container_tag: 16.2_20210521.1
osp_ceph_image: ceph-4.2-rhel-8

# number of OCP worker nodes should be OSP compute hosts
osp_compute_count: 1
# if false nfs is used for glance
osp_compute_hci: false
# number of OSD disks per compute node
osp_compute_ceph_osd_disks:
  - sdb
  - sdc
# size of OSD disk size in GB
osp_compute_ceph_num_osd_disk_size: 20
nfs_server: '192.168.25.1'

# network architecture, either flat or vlan
osp_network_type: vlan

# Local Storage Operator specific (only required if using local-storage operator)
local_storage_domains:
  - "{{ ocp_cluster_name }}_worker_0"
  - "{{ ocp_cluster_name }}_worker_1"
local_storage_workers:
  - worker-0
  - worker-1
local_storage_data_dir: /home/local_storage/data
local_storage_disks:
  - vda
  - vdb
local_storage_disk_size: 50

# Virtualized SRIOV (for dev/test)
enable_virt_sriov: false
virt_sriov_repo: https://github.com/marcel-apf/qemu.git
virt_sriov_domains:
  - "{{ ocp_cluster_name }}_worker_3"
  - "{{ ocp_cluster_name }}_worker_4"
