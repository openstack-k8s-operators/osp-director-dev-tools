---
base_path: /home/ocp
#dev_scripts_repo: defaults to "https://github.com/openshift-metal3/dev-scripts.git"
#dev_scripts_branch: defaults to "HEAD"
# mschuppert: https://github.com/openshift-metal3/dev-scripts/commit/7430bfd427b1e46ad9b2ca79467d62707dc288c0
# deploys python39 and ansible 4.6.0 which makes dev-tools to fail for the short term pin to b8c619c1515d03f162adf08cbb89f9ba5c6d5cf1
dev_scripts_branch: b8c619c1515d03f162adf08cbb89f9ba5c6d5cf1

base_domain_name: test.metalkube.org

# To set a specific release to install.
ocp_version: 4.8
ocp_minor_version: 35
ocp_release_image: "quay.io/openshift-release-dev/ocp-release:{{ ocp_version }}.{{ ocp_minor_version }}-x86_64"
ocp_release_data_url: "https://amd64.ocp.releases.ci.openshift.org/releasestream/4-stable/release/{{ ocp_version }}.{{ ocp_minor_version }}"
ocp_release_type: ga

# If set to true:
# - Assisted installer is used to install OCP instead of dev-scripts
# - Thus, dev-scripts related variables are ignored, and ...
# - ocp_release_type is ignored
# - ocp_version must be 4.6+
# - Other OCP AI variables are stored in vars/ocp_ai.yaml to reduce clutter in this file
# NOTE: This variable should be set to true in local-defaults.yaml as opposed to changing it here,
#       otherwise our playbooks seem to be flaky with executing the AI logic
#ocp_ai: false 

# private repo to read required secret files from
# The playbooks expect secret files which can be placed in a private repo
# Right now these are:
# * rhel-subscription.yaml - content:
#   rhel_subscription_activation_key: <activation key>
#   rhel_subscription_org_id: "xxxxxxx"
#   rhel_subscription_server_hostname: <subscription server> (optional)
# * pull-secret
#   obtain it from https://cloud.redhat.com/openshift/install/pull-secret
#secrets_repo: https uri to repo, no default manual local file is expected if not present
#secrets_branch: defaults to "HEAD"

# dev scripts switched to ipv6 per default, for now switch back
# https://github.com/openshift-metal3/dev-scripts/pull/969
ocp_ip_stack: v4

#
# BEGIN - VM node configuration
#

# size the OCP VM resources
ocp_num_masters: 3
ocp_num_workers: 3
# number of extra VMs to create but not deploy, used as OSP computes
ocp_num_extra_workers: 2
ocp_extra_workers_online_status: false
ocp_master_memory: 16384
ocp_master_vcpu: 8
ocp_master_disk: 30
ocp_worker_memory: 30000
ocp_worker_vcpu: 8
ocp_worker_disk: 50

# the following are used if local storage or OCS is enabled:
# - "ocp_num_storage_workers" must be...
#   * >= 3
#   * <= "ocp_num_workers" if virtual worker nodes are used
#   * <= "ocp_bm_workers|length" if baremetal worker nodes are used
#   * <= "ocp_num_masters" if only virtual master/worker combo nodes are used
#   * <= "ocp_bm_masters|length" if only baremetal master/worker combo nodes are used
#   * <= "ocp_num_masters" or <= "ocp_bm_masters|length" if > "ocp_nm_workers" and > "ocp_bm_workers|length"
# - nodes are enhanced to provide storage along with regular compute (kubelet) functionality
ocp_num_storage_workers: 3
ocp_storage_data_disks:
  - sdb
ocp_storage_data_disk_size: 150
# The following are only used if targetted nodes are virtual
ocp_storage_memory: 51200
ocp_storage_vcpu: 12
ocp_storage_data_dir: /home/local_storage/data

#
# END - VM node configuration
#

#
# BEGIN - BM node configuration (optional)
#
# WARNING: Currently experimental and not fully-tested!
#
# NOTE: Only works with "ocp_ai: true" deployments!
#

# Which interface on the prov host will be attached to the OCP baremetal
# bridge to establish connectivity with the baremetal nodes' OCP network
ocp_bm_interface: ""

# Which interface on the prov host will be attached to the OCP provisioning
# bridge to establish connectivity with the baremetal nodes' provisioning network
ocp_bm_prov_interface: ""

# Which interface on the prov host has connectivity to BMCs on baremetal nodes
ocp_bmc_interface: ""

# Which interface on the prov host will be attached to the OSP network bridge
# to establish connectivity with other baremetal machines on the OSP network
osp_bm_interface: ""

# Which interface on the prov will be attached to the OSP external network bridge
# to establish connectivity with other baremetal machines on the OSP external network
osp_ext_bm_interface: ""

ocp_bm_masters: {}
# ocp_bm_masters:
#   master-0:
#     vendor: Dell
#     bm_interface: ens1f2        # AI uses baremetal (OCP) network interface
#     bm_mac: 40:a6:b7:2b:19:01   # AI uses baremetal (OCP) network MAC
#     bmc_address: 10.10.1.5
#     bmc_username: username
#     bmc_password: password
#     root_device: /dev/sda       # optional, defaults to /dev/sda
#     disabled_interfaces:        # optional, if you need to disable interfaces
#     - name: eno1
#       mac: 34:73:5a:9d:ee:dd
#   master-1:
#     ...
#   master-2:
#     ...
ocp_bm_workers: {}
# ocp_bm_workers:
#   worker-0:
#     vendor: Dell
#     bm_interface: ens1f2        # AI uses baremetal (OCP) network interface
#     bm_mac: 40:a6:b7:2b:20:01   # AI uses baremetal (OCP) network MAC
#     bmc_address: 10.10.1.15
#     bmc_username: username
#     bmc_password: password
#     root_device: /dev/sda       # optional, defaults to /dev/sda
#     disabled_interfaces:        # optional, if you need to disable interfaces
#     - name: eno1
#       mac: 34:73:5a:9d:ee:ff
#   worker-1:
#     ...
ocp_bm_extra_workers: {}
# ocp_bm_extra_workers:
#   worker-2:
#     vendor: Dell
#     bm_mac: 40:a6:b7:2b:20:03   # optional, if for some reason the OSP compute needs an assigned IP on the OCP network
#     prov_mac: 4b:40:40:40:40:01 # extra workers use Metal3, which uses provisioning network MAC
#     bmc_protocol: idrac         # extra workers use Metal3, which need this extra detail
#     bmc_address: 10.10.1.17
#     bmc_username: username
#     bmc_password: password
#     root_device: /dev/sda       # optional, defaults to /dev/sda
#   worker-3:
#     ...

#
# END - BM node configuration (optional)
#

# These are used to simplify considerations for masters and workers
# across virtual and baremetal options.  *** Please leave them as they are ***
### LEAVE AS-IS vvv
ocp_master_count: "{{ ocp_num_masters + ocp_bm_masters|default({})|length }}" # DO NOT EDIT
ocp_worker_count: "{{ ocp_num_workers + ocp_bm_workers|default({})|length }}" # DO NOT EDIT
ocp_extra_worker_count: "{{ ocp_num_extra_workers + ocp_bm_extra_workers|default({})|length }}" # DO NOT EDIT
ocp_cluster_has_bm: "{{ (ocp_bm_masters|default({})|length > 0 or ocp_bm_workers|default({})|length > 0)|bool }}"
### LEAVE AS-IS ^^^

# OCP cluster name
ocp_cluster_name: ostest
ocp_domain_name: "{{ base_domain_name }}"

# Released version of the opm package (can be set to 'latest')
opm_version: v1.12.5

# operator-sdk version to use (must be specific version)
sdk_version: v1.13.1

# golang version
go_version: 1.16.9

# kustomize version to use (must be specific version)
kustomize_version: v4.0.1

# SRIOV network operator version (usually should correspond to X.X release)
sriov_version: "{{ ocp_version }}"

# Performance addon operator version (usually should correspond to X.X release)
perf_version: "{{ ocp_version }}"

# CNV addon operator version (usually should correspond to X.X release)
cnv_hyperconverged_operator_version: "{{ {'4.10':'4.10.1', '4.9':'4.9.4', '4.8':'4.8.6'}.get(ocp_version|string) }}"

# namespace to deploy the operator to
# Note: right now only openstack is supported as it is hardcoded in the Dockerfile
namespace: openstack
watch_namespace: openstack,openshift-machine-api,openshift-sriov-network-operator

# operator github url where operators repos are underneath - default https://github.com/openstack-k8s-operators
#openstack_k8s_operators_https_url: defaults to "https://github.com/openstack-k8s-operators"
#openstack_k8s_operators_director_branch: defaults to "HEAD"

# osp-director-operator image and tag to use
director_operator_image: quay.io/openstack-k8s-operators/osp-director-operator
# CSV version is used for both the CSV version and container image tag
csv_version: v1.2.x-latest

#ocp_network_type: OVNKubernetes
ocp_network_type: OpenShiftSDN

# image used to run tempest
tempest_image: "{{ osp_release_auto.namespace }}/{{ osp_release_auto.name_prefix }}tempest:{{ osp_defaults.container_tag }}"

# tempest timeout in seconds
tempest_timeout: 3600

# run smoketest
tempest_smoketest: false

# enable specific tempest features
tempest_enable_feature_dict:
  compute-feature-enabled:
    vnc_console: true
    live_migration: true
    block_migration_for_live_migration: true
    volume_backed_live_migration: true
    console_output: true

# disable specific tempest features
#tempest_disable_feature_dict:
#  compute-feature-enabled:
#    vnc_console: false

# specify tempest tests to run
# for now phase1 tempest tests
tempest_test_dict:
  regex: ''
  includelist:
      - "tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern"
      - "tempest.scenario.test_minimum_basic.TestMinimumBasicScenario"
      - "tempest.scenario.test_network_basic_ops.TestNetworkBasicOps"
      - "tempest.scenario.test_snapshot_pattern.TestSnapshotPattern"
  # per default with OVN there is no DHCPAgent, disable the tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON tests
  excludelist:
      - "^tempest.scenario.test_network_basic_ops.TestNetworkBasicOps.test_port_security_macspoofing_port"
      - "^tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON.test_add_remove_network_from_dhcp_agent"
      - "^tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON.test_list_networks_hosted_by_one_dhcp"

# phase2 tempest tests
#tempest_test_dict:
#  regex: '(?!.*\[.*\bslow\b.*\])(^tempest\.(api|scenario))'
#  includelist: []
#  # per default with OVN there is no DHCPAgent, disable the tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON tests
#  excludelist:
#      - "^tempest.api.compute.admin.test_auto_allocate_network.AutoAllocateNetworkTest.test_server_multi_create_auto_allocate"
#      - "^tempest.api.compute.admin.test_live_migration.LiveMigrationTest.test_live_block_migration_paused"
#      - "^tempest.api.compute.admin.test_live_migration.LiveAutoBlockMigrationV225Test.test_live_block_migration_paused"
#      - "^tempest.api.compute.admin.test_live_migration.LiveMigrationRemoteConsolesV26Test.test_live_block_migration_paused"
#      - "^tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON.test_add_remove_network_from_dhcp_agent"
#      - "^tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON.test_list_networks_hosted_by_one_dhcp"

local_working_dir: "~/{{ ocp_cluster_name }}-working"

# nfs export directory
nfs_data_dir: /home/nfs/data
nfs_export_dir: /home/nfs

default_timeout: 240

# OSP release and compose file to fetch if specified. This is used to populate osp_release_auto dict
osp_release_auto_version: 16.2-RHEL-8
osp_release_auto_compose: passed_phase2
osp_release_auto_url: http://download.devel.redhat.com/rcm-guest/puddles/OpenStack/{{ osp_release_auto_version }}/{{ osp_release_auto_compose }}/container_image_prepare.yaml

# openstackclient container image
openstackclient_image: "{{ osp_release_auto.namespace }}/{{ osp_release_auto.name_prefix }}tripleoclient:{{ osp_defaults.container_tag }}"

openstackclient_storage_class: host-nfs-storageclass
openstackclient_networks:
  - ctlplane
  - external
  - internal_api

nfs_server: '172.22.0.1'
ntp_server: pool.ntp.org

external_dns:
  - 10.11.5.19
  - 10.38.5.26
  - 10.10.160.2
  - 10.5.30.160

# Set this to the empty string if you don't want an IP placed on the "external"
# bridge (used for OpenStack) created by this tool.  By default this should be
# here for virtualized all-in-one deployments, as the default OSP templates included
# with this tool use this IP on the "external" bridge as the external network gateway
osp_ext_bridge_ip: 10.0.0.1/24

osp_defaults:
  release: "{{ osp_release_auto.release }}"
  # use http://download.devel.redhat.com to get the local mirror depending on where the server is
  base_image_url: http://download.devel.redhat.com/brewroot/packages/rhel-guest-image/8.4/992/images/rhel-guest-image-8.4-992.x86_64.qcow2
  # OSP controller VM sizing
  vmset:
    Controller:
      count: 1
      cores: 6
      disk_size: 40
      memory: 20
      networks:
        - ctlplane
        - internal_api
        - external
        - tenant
        - storage
        - storage_mgmt
      storage_class: host-nfs-storageclass
      storage_access_mode: ReadWriteMany
      storage_volume_mode: Filesystem
  # OSP compute OCP worker settings
  bmset:
    Compute:
      # number of OCP worker nodes should be OSP compute hosts
      count: 2
      ctlplane_interface: enp1s0
      networks:
        - ctlplane
        - internal_api
        - tenant
        - storage
  # number of OSD disks per compute node
  ceph_osd_disks:
    - sdb
    - sdc
  # size of OSD disk size in GB
  ceph_num_osd_disk_size: 20
  domain_name: "osptest.{{ base_domain_name }}"
  dns_servers:
    - 172.22.0.1
  dns_search_domains:
    - "osptest.{{ base_domain_name }}"
    - "{{ base_domain_name }}"
  attach_configs:
    br-ctlplane:
      interfaces:
      - type: bridge
        name: br-ctlplane
        interface: enp1s0
        dhcp: true
        mtu: 1500
    br-osp:
      interfaces:
      - type: bridge
        name: br-osp
        interface: enp7s0
        mtu: 9000
      - type: ethernet
        interface: enp7s0
        mtu: 9000
    br-ex:
      interfaces:
      - type: bridge
        name: br-ex
        interface: enp6s0
        mtu: 1500
  networks: ipv4
  ovn_bridge_mac_mappings:
    phys_networks:
    - name: datacentre
      prefix: fa:16:3a
    - name: datacentre2
      prefix: fa:16:3b
  preserve_reservations: true
  container_tag: "{{ osp_release_auto.tag }}"
  ceph_image: "{{ osp_release_auto.ceph_image }}" # note for OSP17.0 its ceph_image
  ceph_tag: "{{ osp_release_auto.ceph_tag }}" # note for OSP17.0 its ceph_tag
  # OSP deployment timeout
  deploy_timeout: 90m
  # extra features to enable
  # Right now available extra features
  #    - hci
  #    - ipv6
  extrafeatures: []
  # Enable TLS everywhere
  tlse: false
  # Enable TLS public_endpoints
  tls_public_endpoints: true
  # Use IP for TLS public endpoints (vs DNS name)
  tls_public_endpoints_ip: false

# overwrite elements from osp_defaults for a specific release
osp_release_defaults: {}

# overwrite elements from osp_defaults or osp_release_defaults for local settings
osp_local: {}

# registration of the overcloud nodes, either rhsm or rhos-release
osp_registry_method: rhos-release
osp_rhos_release_compose: "{{ osp_release_auto.compose }}"
osp_rhel_subscription_release: "{{ osp_release_auto.rhel_version }}"
osp_rhel_subscription_repos:
  - rhel-8-for-x86_64-baseos-eus-rpms
  - rhel-8-for-x86_64-appstream-eus-rpms
  - rhel-8-for-x86_64-highavailability-eus-rpms
  - ansible-2.9-for-rhel-8-x86_64-rpms
  - advanced-virt-for-rhel-8-x86_64-rpms
  - openstack-{{ osp_defaults.release }}-for-rhel-8-x86_64-rpms
  - fast-datapath-for-rhel-8-x86_64-rpms
  - rhel-8-for-x86_64-nfv-rpms
  - rhceph-4-tools-for-rhel-8-x86_64-rpms
  - codeready-builder-for-rhel-8-x86_64-rpms

# OCS
# NOTE: Only AI deployments currently support OCS, so even if "enable_ocs" is set to true for
#       a dev-scripts deployment, it will not be honored
enable_ocs: false
ocs_version: "{{ 4.8 if ocp_version >= 4.8 else ocp_version }}"

# Virtualized SRIOV (for dev/test)
enable_virt_sriov: false
virt_sriov_repo: https://github.com/marcel-apf/qemu.git
#virt_sriov_branch: defaults to "HEAD"
virt_sriov_domains:
  - "{{ ocp_cluster_name }}_worker_3"
  - "{{ ocp_cluster_name }}_worker_4"

# enable fencing on overcloud controllers
enable_fencing: false
# Note, does not yet work on RHEL9, RHEL9 availability is tracked via https://bugzilla.redhat.com/show_bug.cgi?id=2000954
fencing_agent_packages:
  - fence-agents-common
  - fence-agents-kubevirt

# HTTP Proxy
http_proxy: ""
https_proxy: ""
no_proxy: ""

# Configure freeipa and use for base_domain_name DNS (required for internal TLS)
enable_freeipa: false
freeipa_admin_password: abcd4321
freeipa_directory_manager_password: dcba1234

# must-gather image
must_gather_image: quay.io/openstack-k8s-operators/must-gather:0.0.2
# Optional directory to save must-gather information, defaults to "{{ working_log_dir }}"
# must_gather_directory: "{{ working_log_dir }}"
must_gather_compress_logs: false
