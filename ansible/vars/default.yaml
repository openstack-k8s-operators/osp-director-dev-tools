---
base_path: /home/ocp
# dev_scripts_repo: defaults to "https://github.com/openshift-metal3/dev-scripts.git"
# dev_scripts_branch: defaults to "HEAD"
# mschuppert: https://github.com/openshift-metal3/dev-scripts/commit/7430bfd427b1e46ad9b2ca79467d62707dc288c0
# deploys python39 and ansible 4.6.0 which makes dev-tools to fail for the short term pin to b8c619c1515d03f162adf08cbb89f9ba5c6d5cf1
dev_scripts_branch: b8c619c1515d03f162adf08cbb89f9ba5c6d5cf1

# Set site based on domain name, default to rdu
site_location: "{{
    'tlv' if ansible_nodename | regex_search('tlv.*\\.redhat\\.com') else
    'brq' if ansible_nodename | regex_search('brq.*\\.redhat\\.com') else
    'default'
  }}"

# Site specific settings, local mirrors etc...
site_settings:
  default:
    rhos_release_mirror: rhos-qe-mirror-rdu2.usersys.redhat.com
    dns_servers:
      - 10.11.5.19
      - 10.2.32.1
  brq:
    rhos_release_mirror: rhos-qe-mirror-brq.usersys.redhat.com
    dns_servers:
      - 10.45.248.15
      - 10.38.5.26
  tlv:
    rhos_release_mirror: rhos-qe-mirror-tlv.usersys.redhat.com
    dns_servers:
      - 10.47.242.10
      - 10.38.5.26

# don't change the value, its just here for backward compatability
registry_proxy: "{{ osp_release_auto.registry_proxy }}"

base_domain_name: test.metalkube.org

# registries to add to /etc/containers/registries.conf registries.insecure section
podman_insecure_registries:
  - default-route-openshift-image-registry.apps.{{ ocp_cluster_name }}.{{ base_domain_name }}

# To set a specific release to install.
# TODO: support "latest"/"stable" e.g https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-4.14/
ocp_version: "4.18"
ocp_minor_version: 22
ocp_release_image: "quay.io/openshift-release-dev/ocp-release:{{ ocp_version }}.{{ ocp_minor_version }}-x86_64"
ocp_release_data_url: "https://mirror.openshift.com/pub/openshift-v4/clients/ocp/{{ ocp_version }}.{{ ocp_minor_version }}/release.txt"
ocp_release_type: ga

# OCP cluster wide proxy settings `oc get proxy/cluster`
enable_ocp_cluster_wide_proxy: false
ocp_http_proxy: "http://squid.corp.redhat.com:3128"
ocp_https_proxy: "http://squid.corp.redhat.com:3128"
ocp_no_proxy: ".apps.ostest.{{ base_domain_name }},.usersys.redhat.com,192.168.111.0/24"

# If set to true:
# - Assisted installer is used to install OCP instead of dev-scripts
# - Thus, dev-scripts related variables are ignored, and ...
# - ocp_release_type is ignored
# - ocp_version must be 4.6+
# - Other OCP AI variables are stored in vars/ocp_ai.yaml to reduce clutter in this file
ocp_ai: true

# private repo to read required secret files from
# The playbooks expect secret files which can be placed in a private repo
# Right now these are:
# * rhel-subscription.yaml - content:
#   rhel_subscription_activation_key: <activation key>
#   rhel_subscription_org_id: "xxxxxxx"
#   rhel_subscription_server_hostname: <subscription server> (optional)
# * pull-secret
#   obtain it from https://cloud.redhat.com/openshift/install/pull-secret
# * registry-credentials.yaml (optional) - content:
#   registry_credentials:
#     - registry: registry.foo.io
#       username: <username>
#       password: <password>
# secrets_repo: https uri to repo, no default manual local file is expected if not present
# secrets_branch: defaults to "HEAD"

# dev scripts switched to ipv6 per default, for now switch back
# https://github.com/openshift-metal3/dev-scripts/pull/969
ocp_ip_stack: v4

#
# BEGIN - VM node configuration
#

# size the OCP VM resources
ocp_num_masters: 3
ocp_num_workers: 3
# number of extra VMs to create but not deploy, used as OSP computes
ocp_num_extra_workers: 2
ocp_extra_workers_online_status: false
ocp_master_memory: 16384
ocp_master_vcpu: 8
ocp_master_disk: 70
ocp_worker_memory: 35000
ocp_worker_vcpu: 8
ocp_worker_disk: 50

# the following are used if local storage or OCS is enabled:
# - "ocp_num_storage_workers" must be...
#   * >= 3
#   * <= "ocp_num_workers" if virtual worker nodes are used
#   * <= "ocp_bm_workers | length" if baremetal worker nodes are used
#   * <= "ocp_num_masters" if only virtual master/worker combo nodes are used
#   * <= "ocp_bm_masters | length" if only baremetal master/worker combo nodes are used
#   * <= "ocp_num_masters" or <= "ocp_bm_masters | length" if > "ocp_nm_workers" and > "ocp_bm_workers | length"
# - nodes are enhanced to provide storage along with regular compute (kubelet) functionality
ocp_num_storage_workers: 3
ocp_storage_data_disks:
  - sdb
ocp_storage_data_disk_size: 150
# The following are only used if targetted nodes are virtual
ocp_storage_memory: 51200
ocp_storage_vcpu: 12
ocp_storage_data_dir: /home/local_storage/data

#
# END - VM node configuration
#

#
# BEGIN - BM node configuration (optional)
#
# WARNING: Currently experimental and not fully-tested!
#
# NOTE: Only works with "ocp_ai: true" deployments!
#

# Which interface on the prov host will be attached to the OCP baremetal
# bridge to establish connectivity with the baremetal nodes' OCP network
ocp_bm_interface: ""

# Which interface on the prov host will be attached to the OCP provisioning
# bridge to establish connectivity with the baremetal nodes' provisioning network
ocp_bm_prov_interface: ""

# Which interface on the prov host has connectivity to BMCs on baremetal nodes
ocp_bmc_interface: ""

# Which interface on the prov host will be attached to the OSP network bridge
# to establish connectivity with other baremetal machines on the OSP network
osp_bm_interface: ""

# Which interface on the prov will be attached to the OSP external network bridge
# to establish connectivity with other baremetal machines on the OSP external network
osp_ext_bm_interface: ""

ocp_bm_masters: {}
# ocp_bm_masters:
#   master-0:
#     vendor: Dell
#     bm_interface: ens1f2        # AI uses baremetal (OCP) network interface
#     bm_mac: 40:a6:b7:2b:19:01   # AI uses baremetal (OCP) network MAC
#     bmc_address: 10.10.1.5
#     bmc_username: username
#     bmc_password: password
#     root_device: /dev/sda       # optional, deprecated, use root_device_hints instead, defaults to /dev/sda
#     root_device_hints:          # optional, if you need to specify root device hints
#     - name: hctl
#       value: "0:0:0:0"
#     disabled_interfaces:        # optional, if you need to disable interfaces
#     - name: eno1
#       mac: 34:73:5a:9d:ee:dd
#   master-1:
#     ...
#   master-2:
#     ...
ocp_bm_workers: {}
# ocp_bm_workers:
#   worker-0:
#     vendor: Dell
#     bm_interface: ens1f2        # AI uses baremetal (OCP) network interface
#     bm_mac: 40:a6:b7:2b:20:01   # AI uses baremetal (OCP) network MAC
#     bmc_address: 10.10.1.15
#     bmc_username: username
#     bmc_password: password
#     root_device: /dev/sda       # optional, deprecated, use root_device_hints instead, defaults to /dev/sda
#     root_device_hints:          # optional, if you need to specify root device hints
#     - name: hctl
#       value: "0:0:0:0"
#     disabled_interfaces:        # optional, if you need to disable interfaces
#     - name: eno1
#       mac: 34:73:5a:9d:ee:ff
#   worker-1:
#     ...
ocp_bm_extra_workers: {}
# ocp_bm_extra_workers:
#   worker-2:
#     vendor: Dell
#     bm_mac: 40:a6:b7:2b:20:03       # optional, if for some reason the OSP compute needs an assigned IP on the OCP network
#     prov_mac: 4b:40:40:40:40:01     # extra workers use Metal3, which uses provisioning network MAC
#     bmc_protocol: idrac             # extra workers use Metal3, which need this extra detail
#     bmc_address: 10.10.1.17
#     bmc_username: username
#     bmc_password: password
#     bmc_disable_cert_verify: false  # optional, defaults to false
#     root_device: /dev/sda           # optional, deprecated, use root_device_hints instead, defaults to /dev/sda
#     root_device_hints:              # optional, if you need to specify root device hints
#     - name: hctl
#       value: "0:0:0:0"
#   worker-3:
#     ...

#
# END - BM node configuration (optional)
#

# These are used to simplify considerations for masters and workers
# across virtual and baremetal options.  *** Please leave them as they are ***
### LEAVE AS-IS vvv
ocp_master_count: "{{ ocp_num_masters + ocp_bm_masters | default({}) | length }}" # DO NOT EDIT
ocp_worker_count: "{{ ocp_num_workers + ocp_bm_workers | default({}) | length }}" # DO NOT EDIT
ocp_extra_worker_count: "{{ ocp_num_extra_workers + ocp_bm_extra_workers | default({}) | length }}" # DO NOT EDIT
ocp_cluster_has_bm: "{{ (ocp_bm_masters | default({}) | length > 0 or ocp_bm_workers | default({}) | length > 0) | bool }}"
### LEAVE AS-IS ^^^

# OCP cluster name
ocp_cluster_name: ostest
ocp_domain_name: "{{ base_domain_name }}"

# Released version of the opm package (can be set to 'latest')
opm_version: v1.30.0

# operator-sdk version to use (must be specific version)
# sdk_version: v0.19.2 - cnosp is right now based on that version
sdk_version: v1.20.0

# kuttl version
kuttl_version: 0.12.1

# golang version
go_version: 1.18.9

# kustomize version to use (must be specific version)
kustomize_version: v4.0.1

# SRIOV network operator version stable for 4.14+, OCP version for older
sriov_version: "{{ ocp_version | string if ocp_version | string in ['4.8', '4.9', '4.10', '4.11', '4.12', '4.13'] else 'stable' }}"

# Performance addon operator version (usually should correspond to X.X release)
perf_version: "{{ ocp_version }}"

# CNV addon operator version (usually should correspond to X.X release)
# Current version from https://docs.openshift.com/container-platform/4.14/virt/install/installing-virt.html#installing-virt-operator-cli_installing-virt
cnv_hyperconverged_operator_version: "{{
  {'4.18': '4.18.8',
   '4.16': '4.16.13',
   '4.15': '4.15.10',
   '4.14': '4.14.13',
   '4.13': '4.13.11',
   '4.12': '4.12.18',
   '4.11': '4.11.8',
   '4.10': '4.10.10',
   '4.9': '4.9.7',
   '4.8': '4.8.7'}.get(ocp_version | string)
}}"

# namespace to deploy the operator to
# Note: right now only openstack is supported as it is hardcoded in the Dockerfile
namespace: openstack # noqa: var-naming[no-reserved]
watch_namespace: openstack,openshift-machine-api,openshift-sriov-network-operator

# operator github url where operators repos are underneath - default https://github.com/openstack-k8s-operators
# openstack_k8s_operators_https_url: defaults to "https://github.com/openstack-k8s-operators"
# openstack_k8s_operators_director_branch: defaults to "HEAD"

# osp-director-operator image and tag to use
director_operator_image: quay.io/openstack-k8s-operators/osp-director-operator
# CSV version is used for both the CSV version and container image tag
csv_version: latest-16.2

# ocp_network_type: OpenShiftSDN #for OCP pre 4.16
ocp_network_type: OVNKubernetes

# ansible-sts test suite repo url
sts_repo_url: https://gitlab.cee.redhat.com/openstack-pidone-qe1/openstack-pidone-qe.git

# image used to run tempest
tempest_image: "{{ osp_release_auto.namespace }}/{{ osp_release_auto.name_prefix }}tempest:{{ osp_defaults.container_tag }}"

# tempest timeout in seconds
tempest_timeout: 3600

# run smoketest
tempest_smoketest: false

# enable specific tempest features
tempest_enable_feature_dict:
  compute-feature-enabled:
    vnc_console: true
    live_migration: true
    block_migration_for_live_migration: true
    volume_backed_live_migration: true
    console_output: true

# disable specific tempest features
# tempest_disable_feature_dict:
#   compute-feature-enabled:
#     vnc_console: false

# specify tempest tests to run
# for now phase1 tempest tests
tempest_test_dict:
  regex: ""
  includelist:
    - "tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern"
    - "tempest.scenario.test_minimum_basic.TestMinimumBasicScenario"
    - "tempest.scenario.test_network_basic_ops.TestNetworkBasicOps"
    - "tempest.scenario.test_snapshot_pattern.TestSnapshotPattern"
  # per default with OVN there is no DHCPAgent, disable the tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON tests
  excludelist:
    - "^tempest.scenario.test_network_basic_ops.TestNetworkBasicOps.test_port_security_macspoofing_port"
    - "^tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON.test_add_remove_network_from_dhcp_agent"
    - "^tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON.test_list_networks_hosted_by_one_dhcp"
    # excluding tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_bootable_volume_snapshot_stop_start_instance as it started to
    # fail on latest 16.2 image, using 17,1 would work, but introduce some error on a nother tests.
    # for now lets disable test_bootable_volume_snapshot_stop_start_instance
    - "^tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_bootable_volume_snapshot_stop_start_instance"

# phase2 tempest tests
# tempest_test_dict:
#   regex: '(?!.*\[.*\bslow\b.*\])(^tempest\.(api|scenario))'
#   includelist: []
#   # per default with OVN there is no DHCPAgent, disable the tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON tests
#   excludelist:
#       - "^tempest.api.compute.admin.test_auto_allocate_network.AutoAllocateNetworkTest.test_server_multi_create_auto_allocate"
#       - "^tempest.api.compute.admin.test_live_migration.LiveMigrationTest.test_live_block_migration_paused"
#       - "^tempest.api.compute.admin.test_live_migration.LiveAutoBlockMigrationV225Test.test_live_block_migration_paused"
#       - "^tempest.api.compute.admin.test_live_migration.LiveMigrationRemoteConsolesV26Test.test_live_block_migration_paused"
#       - "^tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON.test_add_remove_network_from_dhcp_agent"
#       - "^tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON.test_list_networks_hosted_by_one_dhcp"

local_working_dir: "~/{{ ocp_cluster_name }}-working"

# nfs export directory
nfs_data_dir: /home/nfs/data
nfs_export_dir: /home/nfs

nfs_pvs:
  - size: 4
    number: 16
  - size: 128
    number: 8
  - size: 128
    number: 10
    shared: true

default_timeout: 240

# git
git:
  user: ospdogit
  group: ospdogit
  port: "4343"

# OSP release and compose file to fetch if specified. This is used to populate osp_release_auto dict
osp_release_auto_version: 16.2-RHEL-8
osp_release_auto_compose: latest_cdn
osp_release_auto_url: https://download.devel.redhat.com/rcm-guest/puddles/OpenStack

# openstackclient container image
openstackclient_image: "{{ osp_release_auto.namespace }}/{{ osp_release_auto.name_prefix }}tripleoclient:{{ osp_defaults.container_tag }}"

openstackclient_storage_class: host-nfs-storageclass
openstackclient_networks:
  - ctlplane
  - external
  - internal_api

nfs_server: "172.22.0.1"
ntp_server: "{{ 'clock.redhat.com' if 'redhat.com' in ansible_fqdn else 'pool.ntp.org' }}"

external_dns: "{{ site_settings[site_location]['dns_servers'] }}"

# Set this to the empty string if you don't want an IP placed on the "external"
# bridge (used for OpenStack) created by this tool.  By default this should be
# here for virtualized all-in-one deployments, as the default OSP templates included
# with this tool use this IP on the "external" bridge as the external network gateway
osp_ext_bridge_ip4: 10.0.0.1/24
osp_ext_bridge_ip6: 2001:db8:fd00:1000::1/64

osp_defaults:
  release: "{{ osp_release_auto.release }}"
  # use https://download.devel.redhat.com to get the local mirror depending on where the server is
  base_image_url: https://download.devel.redhat.com/rhel-8/rel-eng/RHEL-8/latest-RHEL-8.4.0/compose/BaseOS/x86_64/images/rhel-guest-image-8.4-992.x86_64.qcow2
  base_image_add_ca: true
  # OSP controller VM sizing
  vmset:
    Controller:
      count: 1
      cores: 6
      memory: 20
      networks:
        - ctlplane
        - internal_api
        - external
        - tenant
        - storage
        - storage_mgmt
      root_disk:
        disk_size: 40
        storage_class: host-nfs-storageclass
        storage_access_mode: ReadWriteMany
        storage_volume_mode: Filesystem
      additional_disks:
        - name: datadisk
          disk_size: 1
          storage_class: host-nfs-storageclass
          storage_access_mode: ReadWriteMany
          storage_volume_mode: Filesystem
  # OSP compute OCP worker settings
  bmset:
    Compute:
      # number of OCP worker nodes should be OSP compute hosts
      count: 2
      ctlplane_interface: enp1s0
      networks:
        - ctlplane
        - internal_api
        - tenant
        - storage
  # number of OSD disks per compute node
  ceph_osd_disks:
    - sdb
    - sdc
  # size of OSD disk size in GB
  ceph_num_osd_disk_size: 20
  domain_name: "osptest.{{ base_domain_name }}"
  dns_servers:
    - 172.22.0.1
  dns_search_domains:
    - "osptest.{{ base_domain_name }}"
    - "{{ base_domain_name }}"
  attach_configs:
    br-ctlplane:
      interfaces:
        - type: bridge
          name: br-ctlplane
          interface: enp1s0
          dhcp: true
          mtu: 1500
    br-osp:
      interfaces:
        - type: bridge
          name: br-osp
          interface: enp7s0
          mtu: 9000
        - type: ethernet
          interface: enp7s0
          mtu: 9000
    br-ex:
      interfaces:
        - type: bridge
          name: br-ex-osp
          interface: enp6s0
          mtu: 1500
  networks: ipv4
  ovn_bridge_mac_mappings:
    phys_networks:
      - name: datacentre
        prefix: fa:16:3a
      - name: datacentre2
        prefix: fa:16:3b
  preserve_reservations: true
  container_tag: "{{ osp_release_auto.tag }}"
  ceph_image: "{{ osp_release_auto.ceph_image }}" # note for OSP17.0 its ceph_image
  ceph_tag: "{{ osp_release_auto.ceph_tag }}" # note for OSP17.0 its ceph_tag
  # OSP deployment timeout
  deploy_timeout: 90m
  # extra features to enable
  # Right now available extra features
  #    - hci
  #    - ipv6
  extrafeatures: []
  # Any extra IP-hostname combos needed for resolution on overcloud nodes
  extrahostfileentries: []
  # STF host to which to send metrics
  stf_host: ""
  # Enable TLS everywhere
  tlse: false
  # Enable TLS public_endpoints
  tls_public_endpoints: true
  # Use IP for TLS public endpoints (vs DNS name)
  tls_public_endpoints_ip: false

# overwrite elements from osp_defaults for a specific release
osp_release_defaults: {}
# overwrite elements from osp_defaults or osp_release_defaults for local settings
osp_local: {}
# Relative paths to arbitrary extra Heat environment templates to include during OsConfigGenerator generation.
# Relative to /usr/share/openstack-tripleo-heat-templates/environments/.
# Example: ["services/neutron-ovn-sriov.yaml"]
osp_extra_env_files: []
# registration of the overcloud nodes, either rhsm or rhos-release
osp_registry_method: rhos-release
osp_rhos_release: "{{ osp_release_auto.release }}"
osp_rhos_release_mirror: "{{ site_settings[site_location]['rhos_release_mirror'] }}"
osp_rhos_release_compose: "{{ osp_release_auto.compose }}"
osp_rhos_release_compose_rhel8: "{{ osp_release_auto.rhel8_compose }}"
osp_rhel_subscription_release: "{{ osp_release_auto.rhel_version }}"
osp_rhel_subscription_repos:
  - rhel-8-for-x86_64-baseos-eus-rpms
  - rhel-8-for-x86_64-appstream-eus-rpms
  - rhel-8-for-x86_64-highavailability-eus-rpms
  - ansible-2.9-for-rhel-8-x86_64-rpms
  - advanced-virt-for-rhel-8-x86_64-rpms
  - openstack-{{ osp_defaults.release }}-for-rhel-8-x86_64-rpms
  - fast-datapath-for-rhel-8-x86_64-rpms
  - rhel-8-for-x86_64-nfv-rpms
  - rhceph-4-tools-for-rhel-8-x86_64-rpms
  - codeready-builder-for-rhel-8-x86_64-rpms

# Update all packages on the nodes before initial deployment
osp_predeploy_update_packages: true

# OCS
# NOTE: Only AI deployments currently support OCS, so even if "enable_ocs" is set to true for
#       a dev-scripts deployment, it will not be honored
enable_ocs: false
ocs_version: "{{ 4.10 if ocp_version >= 4.10 else ocp_version }}"

# Virtualized SRIOV (for dev/test)
enable_virt_sriov: false
virt_sriov_repo: https://github.com/marcel-apf/qemu.git
# virt_sriov_branch: defaults to "HEAD"
virt_sriov_domains:
  - "{{ ocp_cluster_name }}_worker_3"
  - "{{ ocp_cluster_name }}_worker_4"

# enable fencing on overcloud controllers
enable_fencing: false
# Note, does not yet work on RHEL9, RHEL9 availability is tracked via https://bugzilla.redhat.com/show_bug.cgi?id=2000954
fencing_agent_packages:
  - fence-agents-common
  - fence-agents-kubevirt

# HTTP Proxy
http_proxy: ""
https_proxy: ""
no_proxy: ""

# Configure freeipa and use for base_domain_name DNS (required for internal TLS)
enable_freeipa: false
freeipa_admin_password: abcd4321
freeipa_directory_manager_password: dcba1234
freeipa_image_url: quay.io/freeipa/freeipa-server:centos-9-stream
# must-gather image
must_gather_image: quay.io/openstack-k8s-operators/must-gather:latest
# Optional directory to save must-gather information, defaults to "{{ working_log_dir }}"
# must_gather_directory: "{{ working_log_dir }}"
must_gather_compress_logs: false

cnosp_csv_version: 0.0.1

rhel_repos:
  8:
    - rhel-8-for-x86_64-appstream-rpms
    - advanced-virt-for-rhel-8-x86_64-rpms
    - openstack-16-for-rhel-8-x86_64-rpms
  9:
    - rhel-9-for-x86_64-appstream-rpms
    - codeready-builder-for-rhel-9-x86_64-rpms

rhel_packages:
  8:
  # Directly configured/used in this playbook
    - chrony
    - sysstat
    - cronie
    - tuned
    - firewalld

    # Required for dev-scripts
    - libvirt-daemon-kvm
    - libvirt-client
    - libvirt
    - podman
    - buildah
    - git
    - make

    # Required to run osp-director-operator/scripts/build_and_push_images.sh
    - skopeo

    # Required for assisted installer
    - dnsmasq
    - libvirt-devel
    - python3-netaddr
    - python3-pip
    - qemu-kvm
    - virt-install

    # required to customize the guest-image to remove net.ifnames=0 kernel param
    - libguestfs-tools-c

    # required to run recent sushy-tools
    - python39
    - python39-devel
    - python3-virtualenv
    - gcc

  9:
  # Directly configured/used in this playbook
    - chrony
    - sysstat
    - cronie
    - tuned
    - firewalld

    # Required for dev-scripts
    - libvirt-daemon-kvm
    - libvirt-client
    - libvirt
    - podman
    - buildah
    - git
    - make

    # Required to run osp-director-operator/scripts/build_and_push_images.sh
    - skopeo

    # Required for assisted installer
    - dnsmasq
    - libvirt-devel
    - python3-jmespath
    - python3-netaddr
    - python3-pip
    - python3.11-pip
    - qemu-kvm
    - virt-install

    # required to customize the guest-image to remove net.ifnames=0 kernel param
    - libguestfs-tools-c

    # required to run recent sushy-tools
    - python3
    - python3-devel
    - gcc
