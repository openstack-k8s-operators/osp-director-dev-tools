---
base_path: /home/ocp
#dev_scripts_repo: defaults to "https://github.com/openshift-metal3/dev-scripts.git"
#dev_scripts_branch: defaults to "HEAD"
# mschuppert: https://github.com/openshift-metal3/dev-scripts/commit/7430bfd427b1e46ad9b2ca79467d62707dc288c0
# deploys python39 and ansible 4.6.0 which makes dev-tools to fail for the short term pin to b8c619c1515d03f162adf08cbb89f9ba5c6d5cf1
dev_scripts_branch: b8c619c1515d03f162adf08cbb89f9ba5c6d5cf1

base_domain_name: test.metalkube.org

# To set a specific release to install.
ocp_version: 4.7
ocp_minor_version: 9
ocp_release_image: "quay.io/openshift-release-dev/ocp-release:{{ ocp_version }}.{{ ocp_minor_version }}-x86_64"
ocp_release_type: ga

# If set to true:
# - Assisted installer is used to install OCP instead of dev-scripts
# - Thus, dev-scripts related variables are ignored, and ...
# - ocp_release_type is ignored
# - ocp_version must be 4.7 (for now)
# - ocp_release_image is ignored (for now)
# - Other OCP AI variables are stored in vars/ocp_ai.yaml to reduce clutter in this file
# NOTE: This variable should be set to true in local-defaults.yaml as opposed to changing it here,
#       otherwise our playbooks seem to be flaky with executing the AI logic
#ocp_ai: false 

# private repo to read required secret files from
# The playbooks expect secret files which can be placed in a private repo
# Right now these are:
# * rhel-subscription.yaml - content:
#   rhel_subscription_activation_key: <activation key>
#   rhel_subscription_org_id: "xxxxxxx"
#   rhel_subscription_server_hostname: <subscription server> (optional)
# * pull-secret
#   obtain it from https://cloud.redhat.com/openshift/install/pull-secret
#secrets_repo: https uri to repo, no default manual local file is expected if not present
#secrets_branch: defaults to "HEAD"

# dev scripts switched to ipv6 per default, for now switch back
# https://github.com/openshift-metal3/dev-scripts/pull/969
ocp_ip_stack: v4

# size the OCP VM resources
ocp_num_masters: 3
ocp_num_workers: 3
# number of extra VMs to create but not deploy, used as OSP computes
ocp_num_extra_workers: 2
ocp_extra_workers_online_status: false
ocp_master_memory: 16384
ocp_master_vcpu: 8
ocp_master_disk: 30
ocp_worker_memory: 30000
ocp_worker_vcpu: 8
ocp_worker_disk: 50

# reserved for future expansion. Can be 0 or 1 at this point
ocp_num_bm_workers: 0

# required if ocp_num_bm_workers == 1
#ocp_bm_worker_vendor: Dell
#ocp_bm_worker_bmc_address: 10.16.231.76
#ocp_bm_worker_bmc_user: root
#ocp_bm_worker_bmc_password: calvin
#ocp_bm_worker_mac: 40:a6:b7:2b:19:01
#ocp_bm_worker_ip: 192.168.111.30
#ocp_bm_int: ens3f0

# OCP cluster name
ocp_cluster_name: ostest
ocp_domain_name: "{{ base_domain_name }}"

# Released version of the opm package (can be set to 'latest')
opm_version: v1.12.5

# operator-sdk version to use (must be specific version)
sdk_version: v1.13.1

# golang version
go_version: 1.16.9

# kustomize version to use (must be specific version)
kustomize_version: v4.0.1

# SRIOV network operator version (usually should correspond to X.X release)
sriov_version: "{{ ocp_version }}"

# Performance addon operator version (usually should correspond to X.X release)
perf_version: "{{ ocp_version }}"

# CNV addon operator version (usually should correspond to X.X release)
cnv_hyperconverged_operator_version: 2.6.2

# namespace to deploy the operator to
# Note: right now only openstack is supported as it is hardcoded in the Dockerfile
namespace: openstack
watch_namespace: openstack,openshift-machine-api,openshift-sriov-network-operator

# operator github url where operators repos are underneath - default https://github.com/openstack-k8s-operators
#openstack_k8s_operators_https_url: defaults to "https://github.com/openstack-k8s-operators"
#openstack_k8s_operators_director_branch: defaults to "HEAD"

# osp-director-operator image and tag to use
director_operator_image: quay.io/openstack-k8s-operators/osp-director-operator
# CSV version is used for both the CSV version and container image tag
csv_version: 17.0.1

#ocp_network_type: OVNKubernetes
ocp_network_type: OpenShiftSDN

# image used to run tempest
tempest_image: quay.io/openstack-k8s-operators/rhosp16-openstack-tempest:16.2_20210713.1

# tempest timeout in seconds
tempest_timeout: 3600

# run smoketest
tempest_smoketest: false

# enable specific tempest features
tempest_enable_feature_dict:
  compute-feature-enabled:
    vnc_console: true
    live_migration: true
    block_migration_for_live_migration: true
    volume_backed_live_migration: true
    console_output: true

# disable specific tempest features
#tempest_disable_feature_dict:
#  compute-feature-enabled:
#    vnc_console: false

# specify tempest tests to run
# for now phase1 tempest tests
tempest_test_dict:
  regex: ''
  includelist:
      - "tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern"
      - "tempest.scenario.test_minimum_basic.TestMinimumBasicScenario"
      - "tempest.scenario.test_network_basic_ops.TestNetworkBasicOps"
      - "tempest.scenario.test_snapshot_pattern.TestSnapshotPattern"
  # per default with OVN there is no DHCPAgent, disable the tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON tests
  excludelist:
      - "^tempest.scenario.test_network_basic_ops.TestNetworkBasicOps.test_port_security_macspoofing_port"
      - "^tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON.test_add_remove_network_from_dhcp_agent"
      - "^tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON.test_list_networks_hosted_by_one_dhcp"

# phase2 tempest tests
#tempest_test_dict:
#  regex: '(?!.*\[.*\bslow\b.*\])(^tempest\.(api|scenario))'
#  includelist: []
#  # per default with OVN there is no DHCPAgent, disable the tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON tests
#  excludelist:
#      - "^tempest.api.compute.admin.test_auto_allocate_network.AutoAllocateNetworkTest.test_server_multi_create_auto_allocate"
#      - "^tempest.api.compute.admin.test_live_migration.LiveMigrationTest.test_live_block_migration_paused"
#      - "^tempest.api.compute.admin.test_live_migration.LiveAutoBlockMigrationV225Test.test_live_block_migration_paused"
#      - "^tempest.api.compute.admin.test_live_migration.LiveMigrationRemoteConsolesV26Test.test_live_block_migration_paused"
#      - "^tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON.test_add_remove_network_from_dhcp_agent"
#      - "^tempest.api.network.admin.test_dhcp_agent_scheduler.DHCPAgentSchedulersTestJSON.test_list_networks_hosted_by_one_dhcp"

local_working_dir: "~/{{ ocp_cluster_name }}-working"

# nfs export directory
nfs_data_dir: /home/nfs/data
nfs_export_dir: /home/nfs

default_timeout: 240

# openstackclient container image
openstackclient_image: registry.redhat.io/rhosp-rhel8/openstack-tripleoclient:16.2
openstackclient_storage_class: host-nfs-storageclass
openstackclient_networks:
  - ctlplane
  - external
  - internal_api

nfs_server: '192.168.25.1'

osp_defaults:
  release: 16.2
  # use http://download.devel.redhat.com to get the local mirror depending on where the server is
  base_image_url: http://download.devel.redhat.com/brewroot/packages/rhel-guest-image/8.4/992/images/rhel-guest-image-8.4-992.x86_64.qcow2
  # OSP controller VM sizing
  vmset:
    Controller:
      count: 1
      cores: 6
      disk_size: 40
      memory: 20
      networks:
        - ctlplane
        - internal_api
        - external
        - tenant
        - storage
        - storage_mgmt
      storage_class: host-nfs-storageclass
  # OSP compute OCP worker settings
  bmset:
    Compute:
      # number of OCP worker nodes should be OSP compute hosts
      count: 2
      ctlplane_interface: enp7s0
      networks:
        - ctlplane
        - internal_api
        - external
        - tenant
        - storage
  # number of OSD disks per compute node
  ceph_osd_disks:
    - sdb
    - sdc
  # size of OSD disk size in GB
  ceph_num_osd_disk_size: 20
  domain_name: "osptest.{{ base_domain_name }}"
  dns_servers:
    - 192.168.25.1
  dns_search_domains:
    - "osptest.{{ base_domain_name }}"
    - "{{ base_domain_name }}"
  attach_configs:
    br-osp:
      bridge_name: br-osp
      interface: enp7s0
      mtu: 1500
    br-ex:
      bridge_name: br-ex
      interface: enp6s0
      mtu: 1500
  networks:
    ctlplane:
      name: Control
      name_lower: ctlplane
      is_control_plane: true
      subnets:
      - name: ctlplane
        ipv4:
          cidr: 192.168.25.0/24
          gateway: 192.168.25.1
          allocation:
            start: 192.168.25.100
            end: 192.168.25.250
        attach_config: br-osp
    internalapi:
      name: InternalApi
      name_lower: internal_api
      mtu: 1350
      subnets:
      - name: internal_api
        vlan: 20
        ipv4:
          cidr: 172.17.0.0/24
          allocation:
            start: 172.17.0.10
            end: 172.17.0.250
        attach_config: br-osp
    external:
      name: External
      name_lower: external
      subnets:
      - name: external
        ipv4:
          cidr: 10.0.0.0/24
          gateway: 10.0.0.1
          allocation:
            start: 10.0.0.10
            end: 10.0.0.250
        attach_config: br-ex
    storage:
      name: Storage
      name_lower: storage
      mtu: 1350
      subnets:
      - name: storage
        vlan: 30
        ipv4:
          cidr: 172.18.0.0/24
          allocation:
            start: 172.18.0.10
            end: 172.18.0.250
        attach_config: br-osp
    storagemgmt:
      name: StorageMgmt
      name_lower: storage_mgmt
      mtu: 1350
      subnets:
      - name: storage_mgmt
        vlan: 40
        ipv4:
          cidr: 172.19.0.0/24
          allocation:
            start: 172.19.0.10
            end: 172.19.0.250
        attach_config: br-osp
    tenant:
      name: Tenant
      name_lower: tenant
      vip: false
      mtu: 1350
      subnets:
      - name: tenant
        vlan: 50
        vip: false
        ipv4:
          cidr: 172.20.0.0/24
          allocation:
            start: 172.20.0.10
            end: 172.20.0.250
        attach_config: br-osp
  container_tag: 16.2_20211110.2
  ceph_image: rhceph
  ceph_tag: 4-59
  # OSP deployment timeout
  deploy_timeout: 90m
  # extra features to enable
  # Right now available extra features
  #    - hci
  #    - ipv6
  extrafeatures: []
  # Enable TLS everywhere
  tlse: false

# overwrite elements from osp_defaults for a specific release
osp_release_defaults: {}

# overwrite elements from osp_defaults or osp_release_defaults for local settings
osp_local: {}

# registration of the overcloud nodes, either rhsm or rhos-release
osp_registry_method: rhos-release
osp_rhos_release_compose: passed_phase2
osp_rhel_subscription_release: 8.4
osp_rhel_subscription_repos:
  - rhel-8-for-x86_64-baseos-eus-rpms
  - rhel-8-for-x86_64-appstream-eus-rpms
  - rhel-8-for-x86_64-highavailability-eus-rpms
  - ansible-2.9-for-rhel-8-x86_64-rpms
  - advanced-virt-for-rhel-8-x86_64-rpms
  - openstack-{{ osp_defaults.release }}-for-rhel-8-x86_64-rpms
  - fast-datapath-for-rhel-8-x86_64-rpms
  - rhel-8-for-x86_64-nfv-rpms
  - rhceph-4-tools-for-rhel-8-x86_64-rpms
  - codeready-builder-for-rhel-8-x86_64-rpms

# Local Storage Operator specific (only required if using local-storage operator)
local_storage_domains:
  - "{{ ocp_cluster_name }}_worker_0"
  - "{{ ocp_cluster_name }}_worker_1"
local_storage_workers:
  - worker-0
  - worker-1
local_storage_data_dir: /home/local_storage/data
local_storage_disks:
  - vda
  - vdb
local_storage_disk_size: 50

# Virtualized SRIOV (for dev/test)
enable_virt_sriov: false
virt_sriov_repo: https://github.com/marcel-apf/qemu.git
virt_sriov_domains:
  - "{{ ocp_cluster_name }}_worker_3"
  - "{{ ocp_cluster_name }}_worker_4"

enable_fencing: false

# HTTP Proxy
http_proxy: ""
https_proxy: ""
no_proxy: ""

# Configure freeipa and use for base_domain_name DNS (required for internal TLS)
enable_freeipa: false
freeipa_admin_password: abcd4321
freeipa_directory_manager_password: dcba1234

# must-gather image
must_gather_image: quay.io/openstack-k8s-operators/must-gather:0.0.2
# Optional directory to save must-gather information, defaults to "{{ working_log_dir }}"
# must_gather_directory: "{{ working_log_dir }}"
