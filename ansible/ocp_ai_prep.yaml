---
- hosts: convergence_base
  gather_facts: false
  tasks:
  - name: Include default variables
    include_vars: vars/default.yaml

  - name: Include AI variables
    include_vars: vars/ocp_ai.yaml


  ### ASSISTED INSTALLER CLI BINARY

  - name: Install assisted installer CLI
    become: true
    become_user: root
    pip:
      name: aicli
      version: "{{ ocp_ai_cli_version }}"
      executable: /usr/bin/pip-3

  - name: Install assisted installer CLI - user directory
    pip:
      name: aicli
      version: "{{ ocp_ai_cli_version }}"
      extra_args: "--user"
      executable: /usr/bin/pip-3


  ### ADDITIONAL SET-FACTS

  - name: Set fact for full cluster name
    set_fact:
      ocp_ai_full_cluster_name: "{{ ocp_cluster_name }}.{{ ocp_domain_name | default('test.metalkube.org', true) }}"

  # TODO: Add support for ipv6 for this and the following set_facts
  - name: Set fact for BM CIDR prefix
    set_fact:
      ocp_ai_bm_cidr_prefix: "192.168.111"

  - name: Set fact for reverse BM CIDR suffix
    set_fact:
      ocp_ai_bm_cidr_rev_suffix: "111.168.192"


  ### BRIDGES
  - name: Prepare bridges
    become: true
    become_user: root
    block:
    # FIXME?: Should this task target the playbook host instead?
    - name: Install needed network manager libs
      package:
        name:
          - NetworkManager-libnm
          - nm-connection-editor
        state: present

    - name: Create AI bridges
      block:
      - name: Remove lingering bridges from dev-scripts (if any)
        shell: |
          virsh net-destroy {{ ocp_cluster_name }}{{ item }};
          virsh net-undefine {{ ocp_cluster_name }}{{ item }}
        with_items:
          - bm
          - pr
        register: dev_scripts_br_rm
        failed_when: dev_scripts_br_rm.stderr != "" and "no network with matching name" not in dev_scripts_br_rm.stderr and "is not active" not in dev_scripts_br_rm.stderr

      - name: Delete existing bridges (if any)
        community.general.nmcli:
          conn_name: "{{ item }}"
          type: bridge
          state: absent
        with_items:
          - "{{ ocp_cluster_name }}bm"
          - "{{ ocp_cluster_name }}pr"
          - "ospnetwork"

      - name: Make sure bridge ifcfg files are removed
        file:
          path: "/etc/sysconfig/network-scripts/ifcfg-{{ item }}"
          state: absent
        with_items:
          - "{{ ocp_cluster_name }}bm"
          - "{{ ocp_cluster_name }}pr"
          - "ospnetwork"

      - name: Create BM bridge
        community.general.nmcli:
          conn_name: "{{ ocp_cluster_name }}bm"
          type: bridge
          ifname: "{{ ocp_cluster_name }}bm"
          autoconnect: yes
          stp: off
          # TODO: Support any netmask?
          ip4: "192.168.111.1/24"
          state: present

      - name: Add BM interface to BM bridge
        community.general.nmcli:
          conn_name: "bridge-slave-{{ ocp_bm_int }}"
          type: bridge-slave
          ifname: "{{ ocp_bm_int }}"
          autoconnect: yes
          hairpin: no
          master: "{{ ocp_cluster_name }}bm"
          state: present
        when: ocp_bm_int is defined

      - name: Create provisioning bridge
        community.general.nmcli:
          conn_name: "{{ ocp_cluster_name }}pr"
          type: bridge
          ifname: "{{ ocp_cluster_name }}pr"
          autoconnect: yes
          stp: off
          # TODO: Support any netmask?
          ip4: "172.22.0.1/24"
          state: present

      - name: Reload bridges
        shell: |
          /usr/bin/nmcli con reload {{ ocp_cluster_name }}{{ item }}; /usr/bin/nmcli con up {{ ocp_cluster_name }}{{ item }}
        with_items:
          - bm
          - pr


  ### FIREWALL

  - name: Prepare firewall
    become: true
    become_user: root
    block:
    - name: Acquire default external interface
      shell: |
        ip r | grep default | head -1 | cut -d ' ' -f 5
      register: ocp_ai_ext_intf

    - name: Fail when unable to determine external interface
      fail:
        msg: |
          Unable to determine external interface
      when: ocp_ai_ext_intf.stdout == ""

    - name: Add BM bridge to libvirt zone
      command: "firewall-cmd --zone libvirt --change-interface {{ ocp_cluster_name }}bm --permanent"

    - name: Add TCP firewall rules for BM bridge
      firewalld:
        port: "{{ item }}/tcp"
        state: enabled
        zone: libvirt
        permanent: yes
        immediate: yes
      with_items:
        - 8000
        - 80
        - "{{ ocp_ai_sushy_port | default(8082, true) }}"
        - "{{ ocp_ai_service_port | default(8090, true) }}"
        - 8888

    - name: Add provisioning bridge to libvirt zone
      command: "firewall-cmd --zone libvirt --change-interface {{ ocp_cluster_name }}pr --permanent"

    - name: Add TCP firewall rules for provisioning bridge
      firewalld:
        port: "{{ item }}/tcp"
        state: enabled
        zone: libvirt
        permanent: yes
        immediate: yes
      with_items:
        - 80
        - 2049
        - 5000
        - 5050
        - 6180
        - 6385
        - 8000
        - 9999

    - name: Add UDP firewall rules for provisioning bridge
      firewalld:
        port: "{{ item }}/udp"
        state: enabled
        zone: libvirt
        permanent: yes
        immediate: yes
      with_items:
        - 53
        - 5353
        - 546
        - 547
        - 6230-6239
        - 67
        - 68
        - 69

    # FIXME: Use firewalld rich-rules instead?
    - name: Add direct firewall rules for BM bridge
      shell: |
        firewall-cmd --direct --permanent --add-rule ipv4 nat POSTROUTING 0 -o "{{ ocp_ai_ext_intf.stdout }}" -j MASQUERADE;
        firewall-cmd --direct --permanent --add-rule ipv4 filter FORWARD 0 -i "{{ ocp_cluster_name }}bm" -o "{{ ocp_ai_ext_intf.stdout }}" -j ACCEPT;
        firewall-cmd --direct --permanent --add-rule ipv4 filter FORWARD 0 -i "{{ ocp_ai_ext_intf.stdout }}" -o "{{ ocp_cluster_name }}bm" -m state --state RELATED,ESTABLISHED -j ACCEPT;
        firewall-cmd --reload


  ### HTTP STORE

  - name: Create an HTTP server container to hold ISOs/QCOW2s
    become: true
    become_user: root
    block:
    - name: Create HTTP server storage directory
      file:
        path: /opt/http_store/data/images
        state: directory
        mode: '0777'

    - name: Start httpd container
      containers.podman.podman_container:
        name: httpd
        image: quay.io/openstack-k8s-operators/httpd-24-centos7:2.4
        state: started
        restart: yes
        ports:
          - "80:8080"
        volumes:
          - "/opt/http_store/data:/var/www/html:z"


  ### DNSMASQ

  - name: Prepare dnsmasq
    become: true
    become_user: root
    block:
    - name: Create dnsmasq conf
      template:
        src: "ai/dnsmasq/dnsmasq.conf.j2"
        dest: "/etc/dnsmasq.d/dnsmasq_ai.conf"
        mode: '0644'

    - name: Create NetworkManager dnsmasq DNS conf (to disable it)
      template:
        src: "ai/dnsmasq/nm_dnsmasq.conf.j2"
        dest: "/etc/NetworkManager/conf.d/dnsmasq.conf"
        mode: '0644'

    - name: Restart NetworkManager
      service:
        name: NetworkManager
        state: restarted
        enabled: yes

    - name: Stop all libvirt networks to clear DHCP socket bindings
      shell: |
        #!/bin/bash
        for i in $(virsh net-list | grep -v Autostart | grep -v "---------------" | awk '{print $1}'); do
          virsh net-destroy $i
        done

    - name: Start dnsmasq
      service:
        name: dnsmasq
        state: restarted
        enabled: yes

    - name: Configure /etc/resolv.conf
      template:
          src: "ai/dnsmasq/resolv.conf.j2"
          dest: "/etc/resolv.conf"
          mode: '0644'

  ### VMs

  - name: Provision VMs for use with the assisted installer
    become: true
    become_user: root
    block:
    - name: set fact for total workers
      set_fact:
        total_workers: "{{ ocp_num_workers|int + ocp_num_extra_workers|int }}"

    - name: Delete existing VMs and disk QCOW2s (if any)
      shell: |
        for i in $(virsh list | grep "{{ ocp_cluster_name }}-" | awk '{print $2}'); do
          virsh destroy $i
        done

        for i in $(virsh list --all | grep "{{ ocp_cluster_name }}-" | awk '{print $2}'); do
          virsh undefine --nvram $i
        done

        for i in $(virsh vol-list --pool default  | grep "{{ ocp_cluster_name }}-" | awk '{print $1}'); do
          virsh vol-delete --pool default $i
        done

    # FIXME: Sushy-tools does not allow you to specify the libvirt storage pool, and assumes
    #        that default exists, so we need to make sure that it does
    - name: Check if default storage pool exists with the expected {{ ocp_ai_libvirt_storage_dir }} path
      shell: virsh pool-dumpxml default | grep "<path>{{ ocp_ai_libvirt_storage_dir }}</path>"
      register: libvirt_default_pool
      failed_when: libvirt_default_pool.stderr != "" and "no storage pool with matching name" not in libvirt_default_pool.stderr

    - name: Handle default storage pool
      when: libvirt_default_pool.stdout == ""
      block:
      - name: Render libvirt default storage pool XML
        template:
          src: ai/libvirt/storage-pool.xml.j2
          dest: "/tmp/storage-pool.xml"
          mode: '0664'

      - name: Remove any storage pools currently named "default"
        shell: |
          virsh pool-destroy default
          virsh pool-undefine default
        register: virsh_pool_destroy
        failed_when: virsh_pool_destroy.stderr != "" and "Storage pool not found" not in virsh_pool_destroy.stderr and "storage pool 'default' is not active" not in virsh_pool_destroy.stderr

      - name: Remove any storage pools currently using {{ ocp_ai_libvirt_storage_dir }}
        shell: |
          for i in $(virsh pool-list --all | grep -v "Autostart" | grep -v "\-\-\-\-\-\-\-\-\-" | awk '{print $1}'); do
            if [[ -n '$(virsh pool-dumpxml $i | grep "<path>{{ ocp_ai_libvirt_storage_dir }}</path>")' ]]; then
              virsh pool-destroy $i
              virsh pool-undefine $i
            fi
          done

      - name: Create default storage pool
        shell: |
          virsh pool-define /tmp/storage-pool.xml
          virsh pool-autostart default
          virsh pool-start default


    # Create libvirt volumes for the vm hosts.
    - name: Create master vm storage
      command: >
        virsh vol-create-as 'default'
        '{{ ocp_cluster_name }}-master-{{ item }}'.qcow2 '{{ ocp_master_disk }}'G
        --format qcow2
      with_sequence: start=0 end={{ ocp_num_masters-1 }}

    - name: Create worker vm storage
      command: >
        virsh vol-create-as 'default'
        '{{ ocp_cluster_name }}-worker-{{ item }}'.qcow2 '{{ ocp_worker_disk }}'G
        --format qcow2
      with_sequence: start=0 end={{ total_workers|int-1 if (total_workers|int-1) > 0 else 0 }}
      when: total_workers|int > 0

    - name: define master vms
      vars:
        memory: "{{ ocp_master_memory }}"
        vcpu: "{{ ocp_master_vcpu }}"
        role: master
        prov_bridge_mac_prefix: "{{ ocp_ai_prov_bridge_master_mac_prefix }}"
        bm_bridge_mac_prefix: "{{ ocp_ai_bm_bridge_master_mac_prefix }}"
      virt:
        name: "{{ ocp_cluster_name }}-{{ role }}-{{ item }}"
        command: define
        xml: "{{ lookup('template', 'ai/libvirt/baremetalvm.xml.j2') }}"
        uri: qemu:///system
      with_sequence: start=0 end={{ ocp_num_masters-1 }}

    - name: define worker vms when OCS disabled
      when: (not (enable_ocs | bool)) and total_workers|int > 0
      vars:
        memory: "{{ ocp_worker_memory }}"
        vcpu: "{{ ocp_worker_vcpu }}"
        role: worker
        prov_bridge_mac_prefix: "{{ ocp_ai_prov_bridge_worker_mac_prefix }}"
        bm_bridge_mac_prefix: "{{ ocp_ai_bm_bridge_worker_mac_prefix }}"
      virt:
        name: "{{ ocp_cluster_name }}-{{ role }}-{{ item }}"
        command: define
        xml: "{{ lookup('template', 'ai/libvirt/baremetalvm.xml.j2') }}"
        uri: qemu:///system
      with_sequence: start=0 end={{ total_workers|int-1 if (total_workers|int-1) > 0 else 0 }}

    - name: define worker vms when OCS enabled
      when: (enable_ocs | bool) and total_workers|int > 0
      block:
      - name: define storage worker vms
        vars:
          memory: "{{ ocp_storage_memory }}"
          vcpu: "{{ ocp_storage_vcpu }}"
          role: worker
          prov_bridge_mac_prefix: "{{ ocp_ai_prov_bridge_worker_mac_prefix }}"
          bm_bridge_mac_prefix: "{{ ocp_ai_bm_bridge_worker_mac_prefix }}"
        virt:
          name: "{{ ocp_cluster_name }}-{{ role }}-{{ item }}"
          command: define
          xml: "{{ lookup('template', 'ai/libvirt/baremetalvm.xml.j2') }}"
          uri: qemu:///system
        with_sequence: start=0 end={{ ocp_num_storage_workers-1 }}

      - name: define non-storage and extra worker vms
        when: total_workers|int > 0
        vars:
          memory: "{{ ocp_worker_memory }}"
          vcpu: "{{ ocp_worker_vcpu }}"
          role: worker
          prov_bridge_mac_prefix: "{{ ocp_ai_prov_bridge_worker_mac_prefix }}"
          bm_bridge_mac_prefix: "{{ ocp_ai_bm_bridge_worker_mac_prefix }}"
        virt:
          name: "{{ ocp_cluster_name }}-{{ role }}-{{ item }}"
          command: define
          xml: "{{ lookup('template', 'ai/libvirt/baremetalvm.xml.j2') }}"
          uri: qemu:///system
        with_sequence: start={{ ocp_num_storage_workers if enable_ocs|bool else 0 }} end={{ total_workers|int-1 if (total_workers|int-1) > 0 else 0 }}

    - name: Get worker domain names
      shell: "virsh list --all | grep {{ ocp_cluster_name }}-worker- | awk '{print $2}'"
      register: worker_list

    - name: Print extra worker domain UUIDs
      shell: virsh dumpxml {{ item }} | grep uuid | cut -d '>' -f 2 | cut -d '<' -f 1
      when: index >= ocp_num_workers
      loop: "{{ worker_list.stdout_lines }}"
      loop_control:
        index_var: index
      register: extra_worker_uuids

  ### OCS PREP (if requested)
  - name: OCS and local storage injection, if requested
    when: (enable_ocs | bool)
    block:
    - name: Create VM attached Local Storage for local-storage-operator for AI
      include_role:
        name: local_storage
      vars:
        ocs_local_storage: true
        domain: "{{ ocp_cluster_name }}-worker-{{ item }}"
      loop: "{{ range(0, ocp_num_storage_workers, 1) | list }}"

  ### CRs
  - name: Create Metal3 extra baremetal hosts CRs
    block:
    - name: Include oc_local role
      include_role:
        name: oc_local

    - name: Include default variables
      include_vars: vars/default.yaml

    - name: Set directory for storing AI Metal3 yaml files
      set_fact:
        ai_metal3_yaml_dir: "{{ working_yamls_dir }}/ai_metal3"

    - name: Create local yamldir for AI Metal3 yaml files
      file:
        path: "{{ ai_metal3_yaml_dir }}"
        state: directory
        mode: '0755'

    - name: Render Metal3 extra baremetal hosts CRs
      template:
        src: ai/metal3/extra_workers_bmhs.yml.j2
        dest: "{{ ai_metal3_yaml_dir }}/extra_workers_bmhs.yml"
        mode: '0664'

    delegate_to: localhost
    when: ocp_num_extra_workers > 0


  ### SUSHY-TOOLS

  - name: Install sushy-tools
    become: true
    become_user: root
    block:
    - name: Install sushy-tools via pip3
      pip:
        name: ['sushy-tools', 'libvirt-python']
        executable: /usr/bin/pip-3

    - name: Create sushy-tools conf directory
      file:
        path: /opt/sushy-tools
        state: directory
        mode: '0755'

    - name: Create sushy-tools conf
      template:
        src: ai/sushy-tools/sushy-emulator.conf.j2
        dest: /opt/sushy-tools/sushy-emulator.conf
        mode: '0664'

    - name: Create sushy-tools service
      template:
        src: ai/sushy-tools/sushy-tools.service.j2
        dest: /etc/systemd/system/sushy-tools.service
        mode: '0664'

    - name: Reload systemd service
      systemd:
        daemon_reload: yes

    - name: Start sushy-tools service
      service:
        name: sushy-tools
        state: restarted
        enabled: yes


  ### ASSISTED INSTALLER SERVICE VIA CRUCIBLE

  - name: Register SSH public key
    become: true
    become_user: root
    command: "cat /root/.ssh/id_rsa.pub"
    register: ssh_root_pub_key

  - name: Download and configure assisted installer playbooks
    become: true
    become_user: ocp
    block:
    - name: Pull secret processing
      include_tasks: pull-secret.yaml

    - name: Clone the assisted installer playbooks repo
      git:
        repo: "{{ ocp_ai_ansible_repo | default('https://github.com/openstack-k8s-operators/crucible.git', true) }}"
        dest: "{{ base_path }}/crucible"
        force: true
        version: "{{ ocp_ai_ansible_branch | default('f32b46317e5bb25fffe16c93fdc0f87a72d423bf', true) }}"

    - name: Configure inventory variables
      template:
        src: ai/crucible/inventory.ospd.yml.j2
        dest: "{{ base_path }}/crucible/inventory.ospd.yml"
        mode: '0664'
      vars:
        ssh_pub_key: "{{ ssh_root_pub_key.stdout }}"

    - name: Configure inventory vault variables
      template:
        src: ai/crucible/inventory.vault.ospd.yml.j2
        dest: "{{ base_path }}/crucible/inventory.vault.ospd.yml"
        mode: '0664'

    - name: Create custom registries manifest
      template:
        src: ai/crucible/03-custom-registries.yml.j2
        dest: "{{ base_path }}/crucible/roles/create_cluster/templates/03-custom-registries.yml.j2"
        mode: '0664'

    - name: Override Crucible OCP version constraint
      replace:
        path: "{{ base_path }}/crucible/roles/validate_inventory/tasks/cluster.yml"
        regexp: 'openshift_full_version in supported_ocp_versions'
        replace: 'openshift_full_version is defined'

    - name: Force linear execution of boot_iso role
      block:
      - name: Force linear execution
        replace:
          path: "{{ base_path }}/crucible/playbooks/boot_iso.yml"
          regexp: 'strategy:\s.+'
          replace: 'strategy: linear'

      - name: Force linear execution, part 2
        replace:
          path: "{{ base_path }}/crucible/playbooks/boot_iso.yml"
          regexp: 'serial:\s.+'
          replace: 'serial: 1'

    - name: Remove NTP validation due to prevelance of random false-negatives
      replace:
        path: "{{ base_path }}/crucible/roles/validate_inventory/tasks/network.yml"
        regexp: '\(setup_ntp_service\s\|\sdefault\(True\)\)\s!=\sTrue'
        replace: 'false'

    # Needed until https://github.com/redhat-partner-solutions/crucible/issues/96 is fixed
    - name: Fix "generate SSH keys" check
      replace:
        path: "{{ base_path }}/crucible/deploy_cluster.yml"
        regexp: 'GENERATE_SSH_KEYS'
        replace: 'generate_ssh_keys'

    # Needed until https://github.com/redhat-partner-solutions/crucible/issues/95 is fixed
    - name: Workaround for Crucible "fetched" dir issue
      file:
        path: "{{ base_path }}/crucible/fetched"
        state: directory
        recurse: yes

    - name: Create assisted installer service bash script
      template:
        src: ai/crucible/deploy_ai_service.sh.j2
        dest: "{{ base_path }}/crucible/deploy_ai_service.sh"
        mode: '0755'

    - name: Create assisted installer deploy-cluster bash script
      template:
        src: ai/crucible/deploy_cluster.sh.j2
        dest: "{{ base_path }}/crucible/deploy_cluster.sh"
        mode: '0755'

    - name: Install Crucible Ansible dependencies
      shell: |
        ansible-galaxy collection install -r requirements.yml
      args:
        chdir: "{{ base_path }}/crucible"

    - name: Show assisted service info
      debug:
        msg: |
          Deploying the assisted installer service containers. You can tail the logs at {{ base_path }}/ai_service.log on
          the host for progress.

    - name: Deploy the assisted installer service
      shell: |
        ./deploy_ai_service.sh
      args:
        chdir: "{{ base_path }}/crucible"
      environment:
        ANSIBLE_HOST_KEY_CHECKING: false
