---
- hosts: convergence_base
  gather_facts: false
  tasks:
  - name: Include default variables
    include_vars: vars/default.yaml

  - name: Include AI variables
    include_vars: vars/ocp_ai.yaml


  ### ASSISTED INSTALLER CLI BINARY

  - name: Install assisted installer CLI
    become: true
    become_user: root
    pip:
      name: aicli
      version: "{{ ocp_ai_cli_version }}"
      executable: /usr/bin/pip-3

  - name: Install assisted installer CLI - user directory
    pip:
      name: aicli
      version: "{{ ocp_ai_cli_version }}"
      extra_args: "--user"
      executable: /usr/bin/pip-3

  - name: Install assisted installed CLI lib
    become: true
    become_user: root
    pip:
      name: assisted-service-client
      version: "{{ ocp_ai_cli_lib_version }}"
      executable: /usr/bin/pip-3

  - name: Install assisted installed CLI lib - user directory
    pip:
      name: assisted-service-client
      version: "{{ ocp_ai_cli_lib_version }}"
      extra_args: "--user"
      executable: /usr/bin/pip-3


  ### ADDITIONAL SET-FACTS

  - name: Set fact for full cluster name
    set_fact:
      ocp_ai_full_cluster_name: "{{ ocp_cluster_name }}.{{ ocp_domain_name | default('test.metalkube.org', true) }}"

  # TODO: Add support for ipv6 for this and the following set_facts
  - name: Set fact for BM CIDR prefix
    set_fact:
      ocp_ai_bm_cidr_prefix: "192.168.111"

  - name: Set fact for reverse BM CIDR suffix
    set_fact:
      ocp_ai_bm_cidr_rev_suffix: "111.168.192"

  - name: Set fact for ocp node roles
    set_fact:
      ocp_node_roles:
        - master
        - worker
        - worker-sriov

  ### BRIDGES

  - name: Prepare bridges
    become: true
    become_user: root
    block:
    # FIXME?: Should this task target the playbook host instead?
    - name: Install needed network manager libs
      package:
        name:
          - NetworkManager-libnm
          - nm-connection-editor
        state: present

    - name: Create AI bridges
      block:
      - name: Remove lingering bridges from dev-scripts (if any)
        shell: |
          virsh net-destroy {{ ocp_cluster_name }}{{ item }};
          virsh net-undefine {{ ocp_cluster_name }}{{ item }}
        with_items:
          - bm
          - pr
        register: dev_scripts_br_rm
        failed_when: dev_scripts_br_rm.stderr != "" and "no network with matching name" not in dev_scripts_br_rm.stderr and "is not active" not in dev_scripts_br_rm.stderr

      - name: Delete existing bridges (if any)
        nmcli:
          conn_name: "{{ item }}"
          type: bridge
          state: absent
        with_items:
          - "{{ ocp_cluster_name }}bm"
          - "{{ ocp_cluster_name }}pr"
          - "ospnetwork"

      - name: Make sure bridge ifcfg files are removed
        file:
          path: "/etc/sysconfig/network-scripts/ifcfg-{{ item }}"
          state: absent
        with_items:
          - "{{ ocp_cluster_name }}bm"
          - "{{ ocp_cluster_name }}pr"
          - "ospnetwork"
          - "external"

      - name: Delete existing bridge slaves (if any)
        nmcli:
          conn_name: "bridge-slave-{{ item }}"
          type: bridge-slave
          state: absent
        when: item != ""
        with_items:
          - "{{ ocp_bm_prov_interface }}"
          - "{{ ocp_bm_interface }}"
          - "{{ osp_bm_interface }}"
          - "{{ osp_ext_bm_interface }}"

      - name: Make sure bridge slave ifcfg files are removed
        file:
          path: "/etc/sysconfig/network-scripts/ifcfg-bridge-slave-{{ item }}"
          state: absent
        when: item != ""
        with_items:
          - "{{ ocp_bm_prov_interface }}"
          - "{{ ocp_bm_interface }}"
          - "{{ osp_bm_interface }}"
          - "{{ osp_ext_bm_interface }}"

      - name: Create BM bridge
        nmcli:
          conn_name: "{{ ocp_cluster_name }}bm"
          type: bridge
          ifname: "{{ ocp_cluster_name }}bm"
          autoconnect: yes
          stp: off
          # TODO: Support any netmask?
          ip4: "192.168.111.1/24"
          state: present

      - name: Add BM interface to BM bridge
        when: ocp_bm_interface is defined and ocp_bm_interface != ""
        block:
        - name: Make sure old BM interface connection is down
          shell: |
            /usr/bin/nmcli con down {{ ocp_bm_interface }}
          register: bm_intf_down
          failed_when: bm_intf_down.stderr != "" and "no active connection provided" not in bm_intf_down.stderr

        - name: Add BM interface to BM bridge
          nmcli:
            conn_name: "bridge-slave-{{ ocp_bm_interface }}"
            type: bridge-slave
            ifname: "{{ ocp_bm_interface }}"
            autoconnect: yes
            hairpin: no
            master: "{{ ocp_cluster_name }}bm"
            state: present

      - name: Create provisioning bridge
        nmcli:
          conn_name: "{{ ocp_cluster_name }}pr"
          type: bridge
          ifname: "{{ ocp_cluster_name }}pr"
          autoconnect: yes
          stp: off
          # TODO: Support any netmask?
          ip4: "172.22.0.1/24"
          state: present

      - name: Add prov interface to prov bridge
        when: ocp_bm_prov_interface is defined and ocp_bm_prov_interface != ""
        block:
        - name: Make sure old prov interface connection is down
          shell: |
            /usr/bin/nmcli con down {{ ocp_bm_prov_interface }}
          register: prov_intf_down
          failed_when: prov_intf_down.stderr != "" and "no active connection provided" not in prov_intf_down.stderr

        - name: Add prov interface to prov bridge
          nmcli:
            conn_name: "bridge-slave-{{ ocp_bm_prov_interface }}"
            type: bridge-slave
            ifname: "{{ ocp_bm_prov_interface }}"
            autoconnect: yes
            hairpin: no
            master: "{{ ocp_cluster_name }}pr"
            state: present

      - name: Reload bridges
        shell: |
          /usr/bin/nmcli con reload {{ ocp_cluster_name }}{{ item }}; /usr/bin/nmcli con up {{ ocp_cluster_name }}{{ item }}
        with_items:
          - bm
          - pr


  ### FIREWALL

  - name: Prepare firewall
    become: true
    become_user: root
    block:
    - name: Acquire default external interface
      shell: |
        ip r | grep default | head -1 | cut -d ' ' -f 5
      register: ocp_ai_ext_intf

    - name: Fail when unable to determine external interface
      fail:
        msg: |
          Unable to determine external interface
      when: ocp_ai_ext_intf.stdout == ""

    - name: Add BM bridge to libvirt zone
      command: "firewall-cmd --zone libvirt --change-interface {{ ocp_cluster_name }}bm --permanent"

    - name: Add TCP firewall rules for BM bridge
      firewalld:
        port: "{{ item }}/tcp"
        state: enabled
        zone: libvirt
        permanent: yes
        immediate: yes
      with_items:
        - 8000
        - 80
        - "{{ ocp_ai_sushy_port | default(8082, true) }}"
        - "{{ ocp_ai_service_port | default(8090, true) }}"
        - 8888

    - name: Add provisioning bridge to libvirt zone
      command: "firewall-cmd --zone libvirt --change-interface {{ ocp_cluster_name }}pr --permanent"

    - name: Add TCP firewall rules for provisioning bridge
      firewalld:
        port: "{{ item }}/tcp"
        state: enabled
        zone: libvirt
        permanent: yes
        immediate: yes
      with_items:
        - 80
        - 2049
        - 5000
        - 5050
        - 6180
        - 6385
        - 8000
        - 9999

    - name: Add UDP firewall rules for provisioning bridge
      firewalld:
        port: "{{ item }}/udp"
        state: enabled
        zone: libvirt
        permanent: yes
        immediate: yes
      with_items:
        - 53
        - 5353
        - 546
        - 547
        - 6230-6239
        - 67
        - 68
        - 69

    # FIXME: Use firewalld rich-rules instead?
    - name: Add direct firewall rules for BM bridge
      shell: |
        firewall-cmd --zone libvirt --add-masquerade --permanent
        firewall-cmd --direct --permanent --add-rule ipv4 filter FORWARD 0 -i "{{ ocp_cluster_name }}bm" -o "{{ ocp_ai_ext_intf.stdout }}" -j ACCEPT;
        firewall-cmd --direct --permanent --add-rule ipv4 filter FORWARD 0 -i "{{ ocp_ai_ext_intf.stdout }}" -o "{{ ocp_cluster_name }}bm" -m state --state RELATED,ESTABLISHED -j ACCEPT;
        firewall-cmd --reload


  ### HTTP STORE

  - name: Create an HTTP server container to hold ISOs/QCOW2s
    become: true
    become_user: root
    block:
    - name: Create HTTP server storage directory
      file:
        path: /opt/http_store/data/images
        state: directory
        mode: '0777'

    - name: Start httpd container
      containers.podman.podman_container:
        name: httpd
        image: quay.io/openstack-k8s-operators/httpd-24-centos7:2.4
        state: started
        restart: yes
        ports:
          - "80:8080"
        volumes:
          - "/opt/http_store/data:/var/www/html:z"


  ### DNSMASQ

  - name: Prepare dnsmasq
    become: true
    become_user: root
    block:
    - name: Create dnsmasq conf
      template:
        src: "ai/dnsmasq/dnsmasq.conf.j2"
        dest: "/etc/dnsmasq.d/dnsmasq_ai.conf"
        mode: '0644'

    - name: Create NetworkManager dnsmasq DNS conf (to disable it)
      template:
        src: "ai/dnsmasq/nm_dnsmasq.conf.j2"
        dest: "/etc/NetworkManager/conf.d/dnsmasq.conf"
        mode: '0644'

    - name: Restart NetworkManager
      service:
        name: NetworkManager
        state: restarted
        enabled: yes

    - name: Stop all libvirt networks to clear DHCP socket bindings
      shell: |
        #!/bin/bash
        for i in $(virsh net-list | grep -v Autostart | grep -v "---------------" | awk '{print $1}'); do
          virsh net-destroy $i
        done

    - name: Start dnsmasq
      service:
        name: dnsmasq
        state: restarted
        enabled: yes

    - name: Configure /etc/resolv.conf
      template:
          src: "ai/dnsmasq/resolv.conf.j2"
          dest: "/etc/resolv.conf"
          mode: '0644'

  ### VMs

  - name: Provision VMs for use with the assisted installer
    become: true
    become_user: root
    block:
    - name: set fact for total workers
      set_fact:
        total_workers: "{{ ocp_num_workers|int + ocp_num_extra_workers|int }}"

    - name: Delete existing VMs and disk QCOW2s (if any)
      shell: |
        for i in $(virsh list | grep "{{ ocp_cluster_name }}-" | awk '{print $2}'); do
          virsh destroy $i
        done

        for i in $(virsh list --all | grep "{{ ocp_cluster_name }}-" | awk '{print $2}'); do
          virsh undefine --nvram $i
        done

        for i in $(virsh vol-list --pool default  | grep "{{ ocp_cluster_name }}-" | awk '{print $1}'); do
          virsh vol-delete --pool default $i
        done

    # FIXME: Sushy-tools does not allow you to specify the libvirt storage pool, and assumes
    #        that default exists, so we need to make sure that it does
    - name: Check if default storage pool exists with the expected {{ ocp_ai_libvirt_storage_dir }} path
      shell: virsh pool-dumpxml default | grep "<path>{{ ocp_ai_libvirt_storage_dir }}</path>"
      register: libvirt_default_pool
      failed_when: libvirt_default_pool.stderr != "" and "no storage pool with matching name" not in libvirt_default_pool.stderr

    - name: Handle default storage pool
      when: libvirt_default_pool.stdout == ""
      block:
      - name: Render libvirt default storage pool XML
        template:
          src: ai/libvirt/storage-pool.xml.j2
          dest: "/tmp/storage-pool.xml"
          mode: '0664'

      - name: Remove any storage pools currently named "default"
        shell: |
          virsh pool-destroy default
          virsh pool-undefine default
        register: virsh_pool_destroy
        failed_when: virsh_pool_destroy.stderr != "" and "Storage pool not found" not in virsh_pool_destroy.stderr and "storage pool 'default' is not active" not in virsh_pool_destroy.stderr

      - name: Remove any storage pools currently using {{ ocp_ai_libvirt_storage_dir }}
        shell: |
          for i in $(virsh pool-list --all | grep -v "Autostart" | grep -v "\-\-\-\-\-\-\-\-\-" | awk '{print $1}'); do
            if [[ -n '$(virsh pool-dumpxml $i | grep "<path>{{ ocp_ai_libvirt_storage_dir }}</path>")' ]]; then
              virsh pool-destroy $i
              virsh pool-undefine $i
            fi
          done

      - name: Create default storage pool
        shell: |
          virsh pool-define /tmp/storage-pool.xml
          virsh pool-autostart default
          virsh pool-start default


    # Create libvirt volumes for the vm hosts.
    - name: Create master vm storage
      command: >
        virsh vol-create-as 'default'
        '{{ ocp_cluster_name }}-master-{{ item }}'.qcow2 '{{ ocp_master_disk }}'G
        --format qcow2
      with_sequence: start=0 end={{ ocp_num_masters-1 if ocp_num_masters > 0 else 0 }}
      when: ocp_num_masters > 0

    - name: Create worker vm storage
      command: >
        virsh vol-create-as 'default'
        '{{ ocp_cluster_name }}-worker-{{ item }}'.qcow2 '{{ ocp_worker_disk }}'G
        --format qcow2
      with_sequence: start=0 end={{ total_workers|int-1 if (total_workers|int-1) > 0 else 0 }}
      when: total_workers|int > 0

    - name: define master vms
      vars:
        memory: "{{ ocp_storage_memory if (enable_ocs|bool and ocp_worker_count|int < ocp_num_storage_workers) else ocp_master_memory }}"
        vcpu: "{{ ocp_storage_vcpu if (enable_ocs|bool and ocp_worker_count|int < ocp_num_storage_workers) else ocp_master_vcpu }}"
        role: master
        prov_bridge_mac_prefix: "{{ ocp_ai_prov_bridge_master_mac_prefix }}"
        bm_bridge_mac_prefix: "{{ ocp_ai_bm_bridge_master_mac_prefix }}"
      virt:
        name: "{{ ocp_cluster_name }}-{{ role }}-{{ item }}"
        command: define
        xml: "{{ lookup('template', 'ai/libvirt/baremetalvm.xml.j2') }}"
        uri: qemu:///system
      with_sequence: start=0 end={{ ocp_num_masters-1 if ocp_num_masters > 0 else 0 }}
      when: ocp_num_masters > 0

    - name: define worker vms when OCS disabled
      when: (not (enable_ocs | bool)) and total_workers|int > 0
      vars:
        memory: "{{ ocp_worker_memory }}"
        vcpu: "{{ ocp_worker_vcpu }}"
        role: worker
        prov_bridge_mac_prefix: "{{ ocp_ai_prov_bridge_worker_mac_prefix }}"
        bm_bridge_mac_prefix: "{{ ocp_ai_bm_bridge_worker_mac_prefix }}"
      virt:
        name: "{{ ocp_cluster_name }}-{{ role }}-{{ item }}"
        command: define
        xml: "{{ lookup('template', 'ai/libvirt/baremetalvm.xml.j2') }}"
        uri: qemu:///system
      with_sequence: start=0 end={{ total_workers|int-1 if (total_workers|int-1) > 0 else 0 }}

    - name: define worker vms when OCS enabled
      when: (enable_ocs | bool) and total_workers|int > 0
      block:
      - name: define storage worker vms
        vars:
          memory: "{{ ocp_storage_memory }}"
          vcpu: "{{ ocp_storage_vcpu }}"
          role: worker
          prov_bridge_mac_prefix: "{{ ocp_ai_prov_bridge_worker_mac_prefix }}"
          bm_bridge_mac_prefix: "{{ ocp_ai_bm_bridge_worker_mac_prefix }}"
        virt:
          name: "{{ ocp_cluster_name }}-{{ role }}-{{ item }}"
          command: define
          xml: "{{ lookup('template', 'ai/libvirt/baremetalvm.xml.j2') }}"
          uri: qemu:///system
        with_sequence: start=0 end={{ ocp_num_storage_workers-1 }}
        when: ocp_num_workers >= ocp_num_storage_workers

      - name: define non-storage and extra worker vms
        vars:
          memory: "{{ ocp_worker_memory }}"
          vcpu: "{{ ocp_worker_vcpu }}"
          role: worker
          prov_bridge_mac_prefix: "{{ ocp_ai_prov_bridge_worker_mac_prefix }}"
          bm_bridge_mac_prefix: "{{ ocp_ai_bm_bridge_worker_mac_prefix }}"
        virt:
          name: "{{ ocp_cluster_name }}-{{ role }}-{{ item }}"
          command: define
          xml: "{{ lookup('template', 'ai/libvirt/baremetalvm.xml.j2') }}"
          uri: qemu:///system
        with_sequence: start={{ ocp_num_storage_workers if (enable_ocs|bool and ocp_num_workers >= ocp_num_storage_workers) else 0 }} end={{ total_workers|int-1 if (total_workers|int-1) > 0 else 0 }}

    - name: Get worker domain names
      shell: "virsh list --all | grep {{ ocp_cluster_name }}-worker- | awk '{print $2}'"
      register: worker_list

    - name: Print extra worker domain UUIDs
      shell: virsh dumpxml {{ item }} | grep uuid | cut -d '>' -f 2 | cut -d '<' -f 1
      when: index >= ocp_num_workers
      loop: "{{ worker_list.stdout_lines }}"
      loop_control:
        index_var: index
      register: extra_worker_uuids

  ### OCS PREP (if requested)

  - name: OCS and local storage injection, if requested and only if backed by VMs
    when: (enable_ocs | bool) and
          (
            (ocp_num_masters >= ocp_num_storage_workers and ocp_bm_masters | default({}) | length < 1) 
            or (ocp_num_workers >= ocp_num_storage_workers and ocp_bm_workers | default({}) | length < 1)
          )
    block:
    - name: Create VM attached Local Storage for local-storage-operator for AI
      include_role:
        name: local_storage
      vars:
        ocs_local_storage: true
        domain: "{{ ocp_cluster_name }}-{{ 'worker' if ocp_num_workers >= ocp_num_storage_workers else 'master' }}-{{ item }}"
      loop: "{{ range(0, ocp_num_storage_workers, 1) | list }}"

  ### CRs
  
  - name: Create Metal3 extra baremetal hosts CRs
    block:
    - name: Include oc_local role
      include_role:
        name: oc_local

    - name: Include default variables
      include_vars: vars/default.yaml

    - name: Set directory for storing AI Metal3 yaml files
      set_fact:
        ai_metal3_yaml_dir: "{{ working_yamls_dir }}/ai_metal3"

    - name: Create local yamldir for AI Metal3 yaml files
      file:
        path: "{{ ai_metal3_yaml_dir }}"
        state: directory
        mode: '0755'

    - name: Render Metal3 extra baremetal hosts CRs
      template:
        src: ai/metal3/extra_workers_bmhs.yml.j2
        dest: "{{ ai_metal3_yaml_dir }}/extra_workers_bmhs.yml"
        mode: '0664'

    delegate_to: localhost
    when: ocp_extra_worker_count|int > 0


  ### SUSHY-TOOLS

  - name: Install sushy-tools
    become: true
    become_user: root
    block:
    # latest 4.0.1 fails with: ModuleNotFoundError: No module named 'setuptools_rust'
    - name: Install bcrypt < 4.0.0
      pip:
        name: ['bcrypt']
        version: "<4.0.0"
        executable: /usr/bin/pip-3

    - name: Install sushy-tools via pip3
      pip:
        name: ['sushy-tools', 'libvirt-python']
        executable: /usr/bin/pip-3

    - name: Create sushy-tools conf directory
      file:
        path: /opt/sushy-tools
        state: directory
        mode: '0755'

    - name: Create sushy-tools conf
      template:
        src: ai/sushy-tools/sushy-emulator.conf.j2
        dest: /opt/sushy-tools/sushy-emulator.conf
        mode: '0664'

    - name: Create sushy-tools service
      template:
        src: ai/sushy-tools/sushy-tools.service.j2
        dest: /etc/systemd/system/sushy-tools.service
        mode: '0664'

    - name: Reload systemd service
      systemd:
        daemon_reload: yes

    - name: Start sushy-tools service
      service:
        name: sushy-tools
        state: restarted
        enabled: yes


  ### ASSISTED INSTALLER SERVICE VIA CRUCIBLE

  - name: Register SSH public key
    become: true
    become_user: root
    command: "cat /root/.ssh/id_rsa.pub"
    register: ssh_root_pub_key

  - name: Download and configure assisted installer playbooks
    become: true
    become_user: ocp
    block:
    - name: Pull secret processing
      include_tasks: pull-secret.yaml

    - name: Clone the assisted installer playbooks repo
      git:
        repo: "{{ ocp_ai_ansible_repo | default('https://github.com/redhat-partner-solutions/crucible.git', true) }}"
        dest: "{{ base_path }}/crucible"
        force: true
        version: "{{ ocp_ai_ansible_branch | default('380937382a9929bc3af76724e4a9186cf1d61f50', true) }}"

    - name: Get BMC-network-connected interface's IP for baremetal deployments
      when: ocp_cluster_has_bm|bool
      shell: |
        /usr/bin/nmcli -g ip4.address device show {{ ocp_bmc_interface }} | cut -d '/' -f 1
      register: ocp_bmc_interface_ip

    - name: Configure inventory variables
      template:
        src: ai/crucible/inventory.ospd.yml.j2
        dest: "{{ base_path }}/crucible/inventory.ospd.yml"
        mode: '0664'
      vars:
        ssh_pub_key: "{{ ssh_root_pub_key.stdout }}"

    - name: Configure inventory vault variables
      template:
        src: ai/crucible/inventory.vault.ospd.yml.j2
        dest: "{{ base_path }}/crucible/inventory.vault.ospd.yml"
        mode: '0664'

    - name: Add Crucible template to inject Red Hat CA
      template:
        src: ai/crucible/50-RH-Root-CA.yml.j2
        dest: "{{ base_path }}/crucible/roles/patch_cluster/templates/50-{{ ocp_node_role }}-RH-Root-CA.yml.j2"
        mode: '0664'
      loop_control:
        loop_var: ocp_node_role
      loop: "{{ ocp_node_roles }}"

    - name: Override Crucible OCP version constraint
      replace:
        path: "{{ base_path }}/crucible/roles/validate_inventory/tasks/cluster.yml"
        regexp: 'openshift_full_version in supported_ocp_versions'
        replace: 'openshift_full_version is defined'

    # FIXME: Needed for 4.12 until the fix for https://issues.redhat.com/browse/OCPBUGS-1482 appears in that version
    - name: Remove schedulable_masters from initial cluster creation
      when: ocp_version == "4.12"
      ansible.builtin.lineinfile:
        path: "{{ base_path }}/crucible/roles/create_cluster/tasks/main.yml"
        state: absent
        regexp: '^.*"schedulable_masters"'

    # FIXME: Needed until upstream Crucible supports 4.11 and 4.12
    - name: Inject Crucible support for OCP 4.11/4.12 provisioning images
      when: ocp_version in ["4.11", "4.12"]
      blockinfile:
        path: "{{ base_path }}/crucible/roles/get_image_hash/defaults/main.yml"
        insertafter: "os_images:"
        block: |2
            - openshift_version: '4.11'
              cpu_architecture: x86_64
              url: https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.11/4.11.9/rhcos-live.x86_64.iso
              rootfs_url: https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.11/4.11.9/rhcos-live-rootfs.x86_64.img
              version: 411.86.202210072320-0
            - openshift_version: '4.12'
              cpu_architecture: x86_64
              url: https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.12/4.12.0/rhcos-live.x86_64.iso
              rootfs_url: https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.12/4.12.0/rhcos-live-rootfs.x86_64.img
              version: 412.86.202301061548-0

    # FIXME: Needed until upstream Crucible supports 4.11 and 4.12
    - name: Inject Crucible support for OCP 4.11/4.12 release images
      when: ocp_version in ["4.11", "4.12"]
      blockinfile:
        path: "{{ base_path }}/crucible/roles/get_image_hash/defaults/main.yml"
        insertafter: "release_images:"
        marker: "#*#*#*" # workaround for https://github.com/ansible/ansible/issues/14363
        block: |2
            - openshift_version: '4.11'
              cpu_architecture: x86_64
              url: quay.io/openshift-release-dev/ocp-release:4.11.9-x86_64
              version: 4.11.9
            - openshift_version: '4.12'
              cpu_architecture: x86_64
              url: quay.io/openshift-release-dev/ocp-release:4.12.0-x86_64
              version: 4.12.0

    - name: Override/inject desired OCP version (display_name/release_version)
      replace:
        path: "{{ base_path }}/crucible/roles/get_image_hash/defaults/main.yml"
        regexp: '{{ ocp_version | replace(".", "\.") }}\.\d+'
        replace: '{{ ocp_version }}.{{ ocp_minor_version }}'

    - name: Inject specific RHCOS image version
      block:
      - name: Get RHCOS image version raw data for {{ ocp_version }}.{{ ocp_minor_version }}
        uri:
          url: "{{ ocp_release_data_url }}"
          method: GET
          status_code: [200]
          return_content: True
        register: rhcos_image_version_raw_data

      - name: Get RHCOS image version for {{ ocp_version }}.{{ ocp_minor_version }}
        set_fact:
          rhcos_image_version: "{{ rhcos_image_version_raw_data.content | regex_search('second_release=([^&]+)', '\\1') | default ([''], true) | first }}"
          rhcos_image_version_alt: "{{ rhcos_image_version_raw_data.content | regex_search('\\/\\?release=([^&]+)', '\\1') | default ([''], true) | first }}"

      - name: Fail when an RHCOS version cannot be determined
        fail:
          msg: "Unable to determine RHCOS version for OCP version {{ ocp_version }}.{{ ocp_minor_version }}"
        when: rhcos_image_version == "" and rhcos_image_version_alt == ""

      - name: Inject RHCOS image version
        replace:
          path: "{{ base_path }}/crucible/roles/get_image_hash/defaults/main.yml"
          regexp: 'version:\s"{{ ocp_version | replace(".", "") }}\.\d+.+'
          replace: 'version: "{{ rhcos_image_version if rhcos_image_version != "" else rhcos_image_version_alt }}"'

    - name: Force linear execution of boot_iso role when KVM nodes present
      when: ocp_num_masters > 0 or ocp_num_workers > 0
      block:
      - name: Force linear execution
        replace:
          path: "{{ base_path }}/crucible/playbooks/boot_iso.yml"
          regexp: 'strategy:\s.+'
          replace: 'strategy: linear'

      - name: Force linear execution, part 2
        replace:
          path: "{{ base_path }}/crucible/playbooks/boot_iso.yml"
          regexp: 'serial:\s.+'
          replace: 'serial: 1'

    - name: Remove NTP validation due to prevelance of random false-negatives
      replace:
        path: "{{ base_path }}/crucible/roles/validate_inventory/tasks/network.yml"
        regexp: '\(setup_ntp_service\s\|\sdefault\(True\)\)\s!=\sTrue'
        replace: 'false'

    - name: Create assisted installer service bash script
      template:
        src: ai/crucible/deploy_ai_service.sh.j2
        dest: "{{ base_path }}/crucible/deploy_ai_service.sh"
        mode: '0755'

    - name: Create assisted installer deploy-cluster bash script
      template:
        src: ai/crucible/deploy_cluster.sh.j2
        dest: "{{ base_path }}/crucible/deploy_cluster.sh"
        mode: '0755'

    - name: Install Crucible Ansible dependencies
      shell: |
        ansible-galaxy collection install -r requirements.yml
      args:
        chdir: "{{ base_path }}/crucible"

    - name: Show assisted service info
      debug:
        msg: |
          Deploying the assisted installer service containers. You can tail the logs at {{ base_path }}/ai_service.log on
          the host for progress.

    - name: Deploy the assisted installer service
      shell: |
        ./deploy_ai_service.sh
      args:
        chdir: "{{ base_path }}/crucible"
      environment:
        ANSIBLE_HOST_KEY_CHECKING: false
