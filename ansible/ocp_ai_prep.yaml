---
- hosts: convergence_base
  gather_facts: false
  tasks:
  - name: Include default variables
    include_vars: vars/default.yaml

  - name: Include AI variables
    include_vars: vars/ocp_ai.yaml


  ### ASSISTED INSTALLER CLI BINARY

  - name: Install assisted installer CLI
    become: true
    become_user: root
    pip:
      name: aicli
      executable: /usr/bin/pip-3

  - name: Install assisted installer CLI - user directory
    pip:
      name: aicli
      extra_args: "--user"
      executable: /usr/bin/pip-3


  ### ADDITIONAL SET-FACTS

  - name: Set fact for full cluster name
    set_fact:
      ocp_ai_full_cluster_name: "{{ ocp_cluster_name }}.{{ ocp_domain_name | default('test.metalkube.org', true) }}"

  # TODO: Add support for ipv6 for this and the following set_facts
  - name: Set fact for BM CIDR prefix
    set_fact:
      ocp_ai_bm_cidr_prefix: "192.168.111"

  - name: Set fact for reverse BM CIDR suffix
    set_fact:
      ocp_ai_bm_cidr_rev_suffix: "111.168.192"


  ### BRIDGES
  - name: Prepare bridges
    become: true
    become_user: root
    block:
    # FIXME?: Should this task target the playbook host instead?
    - name: Install needed network manager libs
      package:
        name:
          - NetworkManager-libnm
          - nm-connection-editor
        state: present

    - name: Create AI bridges
      block:
      - name: Remove lingering bridges from dev-scripts (if any)
        shell: |
          virsh net-destroy {{ ocp_cluster_name }}{{ item }};
          virsh net-undefine {{ ocp_cluster_name }}{{ item }}
        with_items:
          - bm
          - pr
        register: dev_scripts_br_rm
        failed_when: dev_scripts_br_rm.stderr != "" and "no network with matching name" not in dev_scripts_br_rm.stderr and "is not active" not in dev_scripts_br_rm.stderr

      - name: Delete existing bridges (if any)
        community.general.nmcli:
          conn_name: "{{ item }}"
          type: bridge
          state: absent
        with_items:
          - "{{ ocp_cluster_name }}bm"
          - "{{ ocp_cluster_name }}pr"
          - "ospnetwork"

      - name: Make sure bridge ifcfg files are removed
        file:
          path: "/etc/sysconfig/network-scripts/ifcfg-{{ item }}"
          state: absent
        with_items:
          - "{{ ocp_cluster_name }}bm"
          - "{{ ocp_cluster_name }}pr"
          - "ospnetwork"

      - name: Create BM bridge
        community.general.nmcli:
          conn_name: "{{ ocp_cluster_name }}bm"
          type: bridge
          ifname: "{{ ocp_cluster_name }}bm"
          autoconnect: yes
          stp: off
          # TODO: Support any netmask?
          ip4: "192.168.111.1/24"
          state: present

      - name: Add BM interface to BM bridge
        community.general.nmcli:
          conn_name: "bridge-slave-{{ ocp_bm_int }}"
          type: bridge-slave
          ifname: "{{ ocp_bm_int }}"
          autoconnect: yes
          hairpin: no
          master: "{{ ocp_cluster_name }}bm"
          state: present
        when: ocp_bm_int is defined

      - name: Create provisioning bridge
        community.general.nmcli:
          conn_name: "{{ ocp_cluster_name }}pr"
          type: bridge
          ifname: "{{ ocp_cluster_name }}pr"
          autoconnect: yes
          stp: off
          # TODO: Support any netmask?
          ip4: "172.22.0.1/24"
          state: present

      - name: Reload bridges
        shell: |
          /usr/bin/nmcli con reload {{ ocp_cluster_name }}{{ item }}; /usr/bin/nmcli con up {{ ocp_cluster_name }}{{ item }}
        with_items:
          - bm
          - pr


  ### FIREWALL

  - name: Prepare firewall
    become: true
    become_user: root
    block:
    - name: Acquire default external interface
      shell: |
        ip r | grep default | head -1 | cut -d ' ' -f 5
      register: ocp_ai_ext_intf

    - name: Fail when unable to determine external interface
      fail:
        msg: |
          Unable to determine external interface
      when: ocp_ai_ext_intf.stdout == ""

    - name: Add BM bridge to libvirt zone
      command: "firewall-cmd --zone libvirt --change-interface {{ ocp_cluster_name }}bm --permanent"

    - name: Add TCP firewall rules for BM bridge
      firewalld:
        port: "{{ item }}/tcp"
        state: enabled
        zone: libvirt
        permanent: yes
        immediate: yes
      with_items:
        - 8000
        - 80
        - "{{ ocp_ai_sushy_port | default(8082, true) }}"
        - "{{ ocp_ai_service_port | default(8090, true) }}"
        - 8888

    - name: Add provisioning bridge to libvirt zone
      command: "firewall-cmd --zone libvirt --change-interface {{ ocp_cluster_name }}pr --permanent"

    - name: Add TCP firewall rules for provisioning bridge
      firewalld:
        port: "{{ item }}/tcp"
        state: enabled
        zone: libvirt
        permanent: yes
        immediate: yes
      with_items:
        - 80
        - 2049
        - 5000
        - 5050
        - 6180
        - 6385
        - 8000
        - 9999

    - name: Add UDP firewall rules for provisioning bridge
      firewalld:
        port: "{{ item }}/udp"
        state: enabled
        zone: libvirt
        permanent: yes
        immediate: yes
      with_items:
        - 53
        - 5353
        - 546
        - 547
        - 6230-6239
        - 67
        - 68
        - 69

    # FIXME: Use firewalld rich-rules instead?
    - name: Add direct firewall rules for BM bridge
      shell: |
        firewall-cmd --direct --permanent --add-rule ipv4 nat POSTROUTING 0 -o "{{ ocp_ai_ext_intf.stdout }}" -j MASQUERADE;
        firewall-cmd --direct --permanent --add-rule ipv4 filter FORWARD 0 -i "{{ ocp_cluster_name }}bm" -o "{{ ocp_ai_ext_intf.stdout }}" -j ACCEPT;
        firewall-cmd --direct --permanent --add-rule ipv4 filter FORWARD 0 -i "{{ ocp_ai_ext_intf.stdout }}" -o "{{ ocp_cluster_name }}bm" -m state --state RELATED,ESTABLISHED -j ACCEPT;
        firewall-cmd --reload


  ### HTTP STORE

  - name: Create an HTTP server container to hold ISOs/QCOW2s
    become: true
    become_user: root
    block:
    - name: Create HTTP server storage directory
      file:
        path: /opt/http_store/data/images
        state: directory
        mode: '0777'

    - name: Start httpd container
      containers.podman.podman_container:
        name: httpd
        image: quay.io/openstack-k8s-operators/httpd-24-centos7:2.4
        state: started
        restart: yes
        ports:
          - "80:8080"
        volumes:
          - "/opt/http_store/data:/var/www/html:z"


  ### DNSMASQ

  - name: Prepare dnsmasq
    become: true
    become_user: root
    block:
    - name: Create dnsmasq conf
      template:
        src: "ai/dnsmasq/dnsmasq.conf.j2"
        dest: "/etc/dnsmasq.d/dnsmasq_ai.conf"
        mode: '0644'

    - name: Create NetworkManager dnsmasq DNS conf (to disable it)
      template:
        src: "ai/dnsmasq/nm_dnsmasq.conf.j2"
        dest: "/etc/NetworkManager/conf.d/dnsmasq.conf"
        mode: '0644'

    - name: Restart NetworkManager
      service:
        name: NetworkManager
        state: restarted
        enabled: yes

    - name: Stop all libvirt networks to clear DHCP socket bindings
      shell: |
        #!/bin/bash
        for i in $(virsh net-list | grep -v Autostart | grep -v "---------------" | awk '{print $1}'); do
          virsh net-destroy $i
        done

    - name: Start dnsmasq
      service:
        name: dnsmasq
        state: restarted
        enabled: yes

    - name: Configure /etc/resolv.conf
      template:
          src: "ai/dnsmasq/resolv.conf.j2"
          dest: "/etc/resolv.conf"
          mode: '0644'

  ### VMs

  - name: Provision VMs for use with the assisted installer
    become: true
    become_user: root
    block:
    - name: Delete existing VMs and disk QCOW2s (if any)
      shell: |
        for i in $(virsh list | grep "{{ ocp_cluster_name }}-" | awk '{print $2}'); do
          virsh destroy $i
        done

        for i in $(virsh list --all | grep "{{ ocp_cluster_name }}-" | awk '{print $2}'); do
          virsh undefine --nvram $i
        done

        for i in $(virsh vol-list --pool default  | grep "{{ ocp_cluster_name }}-" | awk '{print $1}'); do
          virsh vol-delete --pool default $i
        done

    # FIXME: Sushy-tools does not allow you to specify the libvirt storage pool, and assumes
    #        that default exists, so we need to make sure that it does
    - name: Check if default storage pool exists with the expected {{ ocp_ai_libvirt_storage_dir }} path
      shell: virsh pool-dumpxml default | grep "<path>{{ ocp_ai_libvirt_storage_dir }}</path>"
      register: libvirt_default_pool
      failed_when: libvirt_default_pool.stderr != "" and "no storage pool with matching name" not in libvirt_default_pool.stderr

    - name: Handle default storage pool
      when: libvirt_default_pool.stdout == ""
      block:
      - name: Render libvirt default storage pool XML
        template:
          src: ai/libvirt/storage-pool.xml.j2
          dest: "/tmp/storage-pool.xml"
          mode: '0664'

      - name: Remove any storage pools currently named "default"
        shell: |
          virsh pool-destroy default
          virsh pool-undefine default
        register: virsh_pool_destroy
        failed_when: virsh_pool_destroy.stderr != "" and "Storage pool not found" not in virsh_pool_destroy.stderr

      - name: Remove any storage pools currently using {{ ocp_ai_libvirt_storage_dir }}
        shell: |
          for i in $(virsh pool-list --all | grep -v "Autostart" | grep -v "\-\-\-\-\-\-\-\-\-" | awk '{print $1}'); do
            if [[ -n '$(virsh pool-dumpxml $i | grep "<path>{{ ocp_ai_libvirt_storage_dir }}</path>")' ]]; then
              virsh pool-destroy $i
              virsh pool-undefine $i
            fi
          done

      - name: Create default storage pool
        shell: |
          virsh pool-define /tmp/storage-pool.xml
          virsh pool-autostart default
          virsh pool-start default


    # Create libvirt volumes for the vm hosts.
    - name: Create master vm storage
      command: >
        virsh vol-create-as 'default'
        '{{ ocp_cluster_name }}-master-{{ item }}'.qcow2 '{{ ocp_master_disk }}'G
        --format qcow2
      with_sequence: start=0 end={{ ocp_num_masters-1 }}

    - name: Create worker vm storage
      command: >
        virsh vol-create-as 'default'
        '{{ ocp_cluster_name }}-worker-{{ item }}'.qcow2 '{{ ocp_worker_disk }}'G
        --format qcow2
      with_sequence: start=0 end={{ ocp_num_workers+ocp_num_extra_workers-1 }}

    - name: define master vms
      vars:
        memory: "{{ ocp_master_memory }}"
        vcpu: "{{ ocp_master_vcpu }}"
        role: master
        prov_bridge_mac_prefix: "{{ ocp_ai_prov_bridge_master_mac_prefix }}"
        bm_bridge_mac_prefix: "{{ ocp_ai_bm_bridge_master_mac_prefix }}"
      virt:
        name: "{{ ocp_cluster_name }}-{{ role }}-{{ item }}"
        command: define
        xml: "{{ lookup('template', 'ai/libvirt/baremetalvm.xml.j2') }}"
        uri: qemu:///system
      with_sequence: start=0 end={{ ocp_num_masters-1 }}

    - name: define worker vms when OCS disabled
      when: not (enable_ocs | bool)
      vars:
        memory: "{{ ocp_worker_memory }}"
        vcpu: "{{ ocp_worker_vcpu }}"
        role: worker
        prov_bridge_mac_prefix: "{{ ocp_ai_prov_bridge_worker_mac_prefix }}"
        bm_bridge_mac_prefix: "{{ ocp_ai_bm_bridge_worker_mac_prefix }}"
      virt:
        name: "{{ ocp_cluster_name }}-{{ role }}-{{ item }}"
        command: define
        xml: "{{ lookup('template', 'ai/libvirt/baremetalvm.xml.j2') }}"
        uri: qemu:///system
      with_sequence: start=0 end={{ ocp_num_workers+ocp_num_extra_workers-1 }}

    - name: define worker vms when OCS enabled
      when: (enable_ocs | bool)
      block:
      - name: define storage worker vms
        vars:
          memory: "{{ ocp_storage_memory }}"
          vcpu: "{{ ocp_storage_vcpu }}"
          role: worker
          prov_bridge_mac_prefix: "{{ ocp_ai_prov_bridge_worker_mac_prefix }}"
          bm_bridge_mac_prefix: "{{ ocp_ai_bm_bridge_worker_mac_prefix }}"
        virt:
          name: "{{ ocp_cluster_name }}-{{ role }}-{{ item }}"
          command: define
          xml: "{{ lookup('template', 'ai/libvirt/baremetalvm.xml.j2') }}"
          uri: qemu:///system
        with_sequence: start=0 end={{ ocp_num_storage_workers-1 }}

      - name: define non-storage and extra worker vms
        vars:
          memory: "{{ ocp_worker_memory }}"
          vcpu: "{{ ocp_worker_vcpu }}"
          role: worker
          prov_bridge_mac_prefix: "{{ ocp_ai_prov_bridge_worker_mac_prefix }}"
          bm_bridge_mac_prefix: "{{ ocp_ai_bm_bridge_worker_mac_prefix }}"
        virt:
          name: "{{ ocp_cluster_name }}-{{ role }}-{{ item }}"
          command: define
          xml: "{{ lookup('template', 'ai/libvirt/baremetalvm.xml.j2') }}"
          uri: qemu:///system
        with_sequence: start={{ ocp_num_storage_workers }} end={{ ocp_num_workers+ocp_num_extra_workers-1 }}

    - name: Get worker domain names
      shell: "virsh list --all | grep {{ ocp_cluster_name }}-worker- | awk '{print $2}'"
      register: worker_list

    - name: Print extra worker domain UUIDs
      shell: virsh dumpxml {{ item }} | grep uuid | cut -d '>' -f 2 | cut -d '<' -f 1
      when: index >= ocp_num_workers
      loop: "{{ worker_list.stdout_lines }}"
      loop_control:
        index_var: index
      register: extra_worker_uuids

  ### OCS PREP (if requested)
  - name: OCS and local storage injection, if requested
    when: (enable_ocs | bool)
    block:
    - name: Create VM attached Local Storage for local-storage-operator for AI
      include_role:
        name: local_storage
      vars:
        ocs_local_storage: true
        domain: "{{ ocp_cluster_name }}-worker-{{ item }}"
      loop: "{{ range(0, ocp_num_storage_workers, 1) | list }}"

  ### CRs
  - name: Create Metal3 extra baremetal hosts CRs
    block:
    - name: Include oc_local role
      include_role:
        name: oc_local

    - name: Include default variables
      include_vars: vars/default.yaml

    - name: Set directory for storing AI Metal3 yaml files
      set_fact:
        ai_metal3_yaml_dir: "{{ working_yamls_dir }}/ai_metal3"

    - name: Create local yamldir for AI Metal3 yaml files
      file:
        path: "{{ ai_metal3_yaml_dir }}"
        state: directory
        mode: '0755'

    - name: Render Metal3 extra baremetal hosts CRs
      template:
        src: ai/metal3/extra_workers_bmhs.yml.j2
        dest: "{{ ai_metal3_yaml_dir }}/extra_workers_bmhs.yml"
        mode: '0664'

    delegate_to: localhost
    when: ocp_num_extra_workers > 0


  ### SUSHY-TOOLS

  - name: Install sushy-tools
    become: true
    become_user: root
    block:
    - name: Install sushy-tools via pip3
      pip:
        name: ['sushy-tools', 'libvirt-python']
        executable: /usr/bin/pip-3

    - name: Create sushy-tools conf directory
      file:
        path: /opt/sushy-tools
        state: directory
        mode: '0755'

    - name: Create sushy-tools conf
      template:
        src: ai/sushy-tools/sushy-emulator.conf.j2
        dest: /opt/sushy-tools/sushy-emulator.conf
        mode: '0664'

    - name: Create sushy-tools service
      template:
        src: ai/sushy-tools/sushy-tools.service.j2
        dest: /etc/systemd/system/sushy-tools.service
        mode: '0664'

    - name: Reload systemd service
      systemd:
        daemon_reload: yes

    - name: Start sushy-tools service
      service:
        name: sushy-tools
        state: restarted
        enabled: yes


  ### ASSISTED INSTALLER SERVICE SCRIPT

  - name: Prepare assisted installer service
    become: true
    become_user: root
    block:
    - name: Clone the assisted installer service repo
      git:
        repo: "{{ ocp_ai_service_repo | default('https://github.com/sonofspike/assisted-service-onprem.git', true) }}"
        dest: "{{ base_path }}/assisted-service-onprem"
        force: yes
        version: "{{ ocp_ai_service_branch | default('feb00ba10823ab04ec489a05ce8eb543ec688fda', true) }}"

    - name: Create assisted installer service storage directory
      file:
        path: "{{ ocp_ai_service_store_dir | default('/opt/assisted-service', true) }}"
        state: directory
        mode: '0755'

    - name: Set assisted installer service storage directory
      replace:
        path: "{{ base_path }}/assisted-service-onprem/start-assisted-service.sh"
        regexp: 'OAS_HOSTDIR=.*'
        replace: 'OAS_HOSTDIR={{ ocp_ai_service_store_dir | default("/opt/assisted-service", true) }}'

    - name: Set assisted installer service discovery ISO location
      replace:
        path: "{{ base_path }}/assisted-service-onprem/start-assisted-service.sh"
        regexp: 'BASE_OS_IMAGE=.*'
        replace: 'BASE_OS_IMAGE={{ ocp_ai_discovery_iso }}'

    - name: Set assisted installer service base URL
      replace:
        path: "{{ base_path }}/assisted-service-onprem/onprem-environment"
        regexp: 'SERVICE_BASE_URL=.*'
        replace: 'SERVICE_BASE_URL=http://192.168.111.1:{{ ocp_ai_service_port | default("8090", true) }}'

    - name: Create dict of upstream AI onprem variables
      set_fact:
        ocp_ai_onprem_env_dict: "{{ ocp_ai_onprem_env_dict|default({}) | combine( {item.split('=')[0]: item.split('=')[1]} ) }}"
      with_items:
        - "{{ ocp_ai_onprem_env }}"

    - name: Remove deprecated OPENSHIFT_VERSION onprem env var
      replace:
        path: "{{ base_path }}/assisted-service-onprem/onprem-environment"
        regexp: 'OPENSHIFT_VERSIONS=.*'
        replace: ''

    - name: Inject OCP release image onprem env vars
      lineinfile:
        path: "{{ base_path }}/assisted-service-onprem/onprem-environment"
        state: present
        line: '{{ item }}={{ ocp_ai_onprem_env_dict[item] }}'
      with_items:
        - OS_IMAGES
        - RELEASE_IMAGES

    - name: Set assisted installer service default hardware requirements # noqa var-spacing
      ansible.builtin.lineinfile:
        path: "{{ base_path }}/assisted-service-onprem/onprem-environment"
        regexp: '^HW_VALIDATOR_REQUIREMENTS='
        line: 'HW_VALIDATOR_REQUIREMENTS=[{"version":"default","master":{"cpu_cores":2,"ram_mib":8192,"disk_size_gb":30,"installation_disk_speed_threshold_ms":10},"worker":{"cpu_cores":2,"ram_mib":8192,"disk_size_gb":30,"installation_disk_speed_threshold_ms":10},"sno":{"cpu_cores":8,"ram_mib":32768,"disk_size_gb":30,"installation_disk_speed_threshold_ms":10}}]'

    - name: Start assisted installer service container
      shell: |
        ./stop-assisted-service.sh;
        ./start-assisted-service.sh
      args:
        chdir: "{{ base_path }}/assisted-service-onprem"

    - name: Wait until the assisted installer service is fully available
      shell: |
        aicli -U http://192.168.111.1:{{ ocp_ai_service_port | default('8090', true) }} list cluster
      register: assisted_service_ready
      until: '"Dns Domain" in assisted_service_ready.stdout'
      retries: 60
      delay: 10
      environment:
        PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin


  ### ASSISTED INSTALLER EXECUTION PLAYBOOKS

  # Work-around as invoking lookup plugin inside inventory.ospd.j2 template
  # fails. This is because ansible user is different then id_rsa.pub one.
  - name: Register SSH public key
    become: true
    become_user: root
    command: "cat /root/.ssh/id_rsa.pub"
    register: ssh_root_pub_key

  - name: Download and configure assisted installer playbooks
    become: true
    become_user: ocp
    block:
    - name: Pull secret processing
      include_tasks: pull-secret.yaml

    - name: Clone the assisted installer playbooks repo
      git:
        repo: "{{ ocp_ai_ansible_repo | default('https://github.com/openstack-k8s-operators/cluster_mgnt_roles.git', true) }}"
        dest: "{{ base_path }}/cluster_mgnt_roles"
        force: true
        version: "{{ ocp_ai_ansible_branch | default('master', true) }}"

    # HACK: Remove old, bad logic from SoS so that it works with latest AI service (we never needed it anyhow)
    # Once this logic is fixed upstream in SoS, we can remove this task
    - name: Remove problematic SoS create-cluster logic
      replace:
        path: "{{ base_path }}/cluster_mgnt_roles/create_cluster/tasks/main.yml"
        after: '##### TO REMOVE ####'
        before: '#TODO: Apply manifests before cluster installation'
        regexp: '^(.+)$'
        replace: '# \1'

    - name: Add schedulable-master manifest
      block:
      - name: Create schedulable-master manifest
        template:
          src: ai/cluster_mgnt_roles/50-master-scheduler.yml.j2
          dest: "{{ base_path }}/cluster_mgnt_roles/create_cluster/templates/50-master-scheduler.yml.j2"
          mode: '0664'

      - name: Inject schedulable-master manifest
        lineinfile:
          path: "{{ base_path }}/cluster_mgnt_roles/create_cluster/tasks/main.yml"
          regexp: '^\s\s\s\s-\s50-master-scheduler\.yml'
          insertafter: '^\s\s\s\s-\s50-worker-remove-ipi-leftovers\.yml'
          line: '    - 50-master-scheduler.yml'

    - name: Add ingress controller manifest
      block:
      - name: Create ingress controller manifest
        template:
          src: ai/cluster_mgnt_roles/60-ingress-controller.yml.j2
          dest: "{{ base_path }}/cluster_mgnt_roles/create_cluster/templates/60-ingress-controller.yml.j2"
          mode: '0664'

      - name: Inject ingress controller manifest
        lineinfile:
          path: "{{ base_path }}/cluster_mgnt_roles/create_cluster/tasks/main.yml"
          regexp: '^\s\s\s\s-\s60-ingress-controller\.yml'
          insertafter: '^\s\s\s\s-\s50-master-scheduler\.yml'
          line: '    - 60-ingress-controller.yml'

    - name: Force serial execution of boot_iso role
      replace:
        after: "Mounting, Booting the Assisted Installer Discovery ISO"
        before: "- boot_iso"
        path: "{{ base_path }}/cluster_mgnt_roles/deploy_cluster.yml"
        regexp: 'strategy: free'
        replace: 'serial: 1'

    - name: Configure inventory variables
      template:
          src: ai/cluster_mgnt_roles/inventory.ospd.j2
          dest: "{{ base_path }}/cluster_mgnt_roles/inventory.ospd"
          mode: '0664'
      vars:
          ssh_pub_key: "{{ ssh_root_pub_key.stdout }}"
