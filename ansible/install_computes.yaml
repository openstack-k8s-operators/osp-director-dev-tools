---
- hosts: localhost
  vars_files: vars/default.yaml
  roles:
  - oc_local

  tasks:
  - name: Set combined osp dict
    set_fact:
      osp: "{{ osp_defaults | combine((osp_release_defaults | default({})), recursive=True) | combine((osp_local | default({})), recursive=True) }}"

  - name: Set directory for compute yaml files
    set_fact:
      compute_yaml_dir: "{{ working_yamls_dir }}/compute"

  - name: show yaml output dir
    debug:
      msg: "yamls will be written to {{ compute_yaml_dir }} locally"

  - name: Clean yaml dir
    file:
      state: absent
      path: "{{ compute_yaml_dir }}/"

  - name: Create yaml dir
    file:
      path: "{{ compute_yaml_dir }}"
      state: directory
      mode: '0755'

  - name: Ensure that {{ base_path }}/pool exists
    file:
      path: "{{ base_path }}/pool"
      state: directory
      mode: '0755'
    become: true
    become_user: root

  - name: Detach additional Ceph OSD disks
    shell: |
      #!/bin/bash
      for domain in $(virsh list --inactive --name); do
        for disk in $(virsh domblklist ${domain} | grep ceph | awk '{print $1}'); do
          virsh detach-disk  ${domain} ${disk} --persistent
          rm -f {{ base_path }}/pool/${domain}-ceph-disk-${disk}.qcow2
        done
      done
    become: true
    become_user: root

  - name: Create additional Ceph OSD disks
    shell: |
      #!/bin/bash
      for domain in $(virsh list --inactive --name); do
        qemu-img create -f qcow2 -o preallocation=falloc {{ base_path }}/pool/${domain}-ceph-disk-{{ item }}.qcow2 {{ osp.ceph_num_osd_disk_size }}G
        virsh attach-disk ${domain} --source {{ base_path }}/pool/${domain}-ceph-disk-{{ item }}.qcow2 --target {{ item }} --persistent
      done
    with_items: "{{ osp.ceph_osd_disks }}"
    become: true
    become_user: root

  - name: register dev-scripts prepared extra_hosts
    shell: |
      set -e
      oc apply -f {{ base_path }}/dev-scripts/ocp/ostest/extra_host_manifests.yaml
    environment:
      PATH: "{{ oc_env_path }}"
      KUBECONFIG: "{{ kubeconfig }}"
    when: not (ocp_ai|bool)

  - name: Include AI variables
    include_vars: vars/ocp_ai.yaml

  - name: Register assisted installer prepared extra_hosts
    shell: |
      set -e
      oc apply -f {{ working_yamls_dir }}/ai_metal3/extra_workers_bmhs.yml
    environment:
      PATH: "{{ oc_env_path }}"
      KUBECONFIG: "{{ kubeconfig }}"
    when: ocp_ai|bool and ocp_num_extra_workers > 0

  # render openstackbaremetalset.yaml per bmset
  - name: Render templates to yaml dir
    template:
      src: "osp/compute/openstackbaremetalset.yaml.j2"
      dest: "{{ compute_yaml_dir }}/{{ _role }}_openstackbaremetalset.yaml"
      mode: '0644'
    loop: "{{ osp.bmset.keys() | list }}"
    loop_control:
      loop_var: _role

  - name: Render provisionserver template to yaml dir
    template:
      src: "osp/compute/openstackprovisionserver.yaml.j2"
      dest: "{{ compute_yaml_dir }}/openstackprovisionserver.yaml"
      mode: '0644'

  - name: does the provisionserver already exist
    shell: >
      oc get -n {{ namespace }} osprovserver openstack -o json --ignore-not-found
    environment: &oc_env
      PATH: "{{ oc_env_path }}"
      KUBECONFIG: "{{ kubeconfig }}"
    register: provisionserver_switch

  - name: Start provisionserver
    shell: |
      set -e
      oc apply -n {{ namespace }} -f "{{ compute_yaml_dir }}/openstackprovisionserver.yaml"
    environment:
      <<: *oc_env
    when: provisionserver_switch.stdout | length == 0

  - name: Start all baremetalsets
    shell: |
      set -e
      oc apply -n {{ namespace }} -f "{{ item }}"
    environment:
      <<: *oc_env
    with_fileglob:
    - "{{ compute_yaml_dir }}/*_openstackbaremetalset.yaml"
